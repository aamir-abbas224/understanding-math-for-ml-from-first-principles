{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# Linear Algebra for AI/ML - Part 4: Projections, Orthogonality & Eigenvalues\n",
    "\n",
    "This notebook covers projection matrices, orthogonality concepts, and eigenvalue decomposition.\n",
    "\n",
    "**Prerequisites:** Complete Parts 1-3 first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "setup",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NumPy version: 2.2.6\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Setup: Import Required Libraries\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import linalg\n",
    "\n",
    "np.set_printoptions(precision=4, suppress=True)\n",
    "print(f\"NumPy version: {np.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "projection-header",
   "metadata": {},
   "source": [
    "## 24. Projection Matrix\n",
    "\n",
    "A projection matrix P projects vectors onto a subspace.\n",
    "\n",
    "**Properties:**\n",
    "1. P² = P (idempotent)\n",
    "2. Pᵀ = P (symmetric)\n",
    "\n",
    "**Formula:** P = A(AᵀA)⁻¹Aᵀ\n",
    "\n",
    "**ML Application:** Linear regression, PCA, orthogonal projections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "projection",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Projection onto a Line\n",
      "============================================================\n",
      "\n",
      "Line direction: a = [3 1]\n",
      "\n",
      "Projection matrix P:\n",
      "[[0.9 0.3]\n",
      " [0.3 0.1]]\n",
      "\n",
      "------------------------------------------------------------\n",
      "Verifying Projection Properties\n",
      "------------------------------------------------------------\n",
      "\n",
      "1. Idempotence: P² = P\n",
      "   P²:\n",
      "   [[0.9 0.3]\n",
      " [0.3 0.1]]\n",
      "   P:\n",
      "   [[0.9 0.3]\n",
      " [0.3 0.1]]\n",
      "   Equal? True ✓\n",
      "\n",
      "2. Symmetry: Pᵀ = P\n",
      "   Pᵀ:\n",
      "   [[0.9 0.3]\n",
      " [0.3 0.1]]\n",
      "   Equal? True ✓\n",
      "\n",
      "------------------------------------------------------------\n",
      "Projecting Vectors\n",
      "------------------------------------------------------------\n",
      "\n",
      "Original vector v: [4 3]\n",
      "Projected vector Pv: [4.5 1.5]\n",
      "\n",
      "Pv = 1.5000 × a\n",
      "Verify: 1.5000 × [3 1] = [4.5 1.5]\n",
      "Match? True ✓\n",
      "\n",
      "Perpendicular component: v - Pv = [-0.5  1.5]\n",
      "\n",
      "Orthogonality check:\n",
      "(v - Pv) · Pv = 0.0000000000\n",
      "Is orthogonal? True ✓\n",
      "\n",
      "============================================================\n",
      "Projection onto a Plane in ℝ³\n",
      "============================================================\n",
      "\n",
      "Plane spanned by:\n",
      "a₁ = [1 0 0]\n",
      "a₂ = [0 1 0]\n",
      "\n",
      "Matrix A = [a₁ a₂]:\n",
      "[[1 0]\n",
      " [0 1]\n",
      " [0 0]]\n",
      "\n",
      "Projection matrix P:\n",
      "[[1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 0.]]\n",
      "\n",
      "Vector v = [2 3 5]\n",
      "Projection Pv = [2. 3. 0.]\n",
      "\n",
      "Note: z-component is removed (projected onto xy-plane) ✓\n",
      "\n",
      "Verify P² = P:\n",
      "Equal? True ✓\n",
      "\n",
      "============================================================\n",
      "ML Application: Linear Regression as Projection\n",
      "============================================================\n",
      "\n",
      "Data:\n",
      "  X shape: (50, 2)\n",
      "  y shape: (50,)\n",
      "  True weights: [ 3 -2]\n",
      "\n",
      "Estimated weights β: [ 2.9428 -2.0237]\n",
      "Close to true? True ✓\n",
      "\n",
      "Hat matrix P (projection matrix):\n",
      "Shape: (50, 50)\n",
      "\n",
      "Predictions:\n",
      "ŷ = Py (projection of y onto column space of X)\n",
      "First 5 predictions: [ 1.7415 -1.1762 -0.2152  3.0942 -2.4795]\n",
      "First 5 actual: [ 1.059  -1.3133 -0.4055  2.8016 -2.5742]\n",
      "\n",
      "Residuals: e = y - ŷ = (I - P)y\n",
      "First 5 residuals: [-0.6825 -0.1371 -0.1903 -0.2925 -0.0947]\n",
      "\n",
      "Orthogonality: Xᵀe should be ≈ 0\n",
      "Xᵀe = [-0. -0.]\n",
      "All ≈ 0? True ✓\n",
      "\n",
      "→ Linear regression IS a projection!\n",
      "→ Projects y onto space spanned by X columns\n",
      "→ Residuals perpendicular to column space\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Projection Matrix\n",
    "\n",
    "Purpose:\n",
    "Projects any vector onto a subspace (e.g., line, plane)\n",
    "Keeps the component in the subspace, discards the perpendicular part\n",
    "\n",
    "Formula:\n",
    "For subspace spanned by columns of A:\n",
    "    P = A(AᵀA)⁻¹Aᵀ\n",
    "\n",
    "Properties:\n",
    "1. P² = P (applying twice = applying once)\n",
    "2. Pᵀ = P (symmetric)\n",
    "3. Eigenvalues are 0 or 1 only\n",
    "4. rank(P) = dimension of subspace\n",
    "\n",
    "ML Applications:\n",
    "- Linear regression: ŷ = Py\n",
    "- PCA: project onto principal components\n",
    "- Removing components (I - P)\n",
    "\"\"\"\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"Projection onto a Line\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Define a line (1D subspace) in ℝ²\n",
    "a = np.array([[3], [1]])  # Direction vector (column)\n",
    "\n",
    "print(f\"\\nLine direction: a = {a.ravel()}\")\n",
    "\n",
    "# Projection matrix onto line spanned by a\n",
    "# P = a(aᵀa)⁻¹aᵀ = (aaᵀ)/(aᵀa)\n",
    "P = (a @ a.T) / (a.T @ a)\n",
    "\n",
    "print(f\"\\nProjection matrix P:\")\n",
    "print(P)\n",
    "\n",
    "# Verify properties\n",
    "print(\"\\n\" + \"-\"*60)\n",
    "print(\"Verifying Projection Properties\")\n",
    "print(\"-\"*60)\n",
    "\n",
    "# Property 1: P² = P\n",
    "P_squared = P @ P\n",
    "print(f\"\\n1. Idempotence: P² = P\")\n",
    "print(f\"   P²:\")\n",
    "print(f\"   {P_squared}\")\n",
    "print(f\"   P:\")\n",
    "print(f\"   {P}\")\n",
    "print(f\"   Equal? {np.allclose(P_squared, P)} ✓\")\n",
    "\n",
    "# Property 2: Pᵀ = P\n",
    "P_transpose = P.T\n",
    "print(f\"\\n2. Symmetry: Pᵀ = P\")\n",
    "print(f\"   Pᵀ:\")\n",
    "print(f\"   {P_transpose}\")\n",
    "print(f\"   Equal? {np.allclose(P_transpose, P)} ✓\")\n",
    "\n",
    "# Project a vector\n",
    "print(\"\\n\" + \"-\"*60)\n",
    "print(\"Projecting Vectors\")\n",
    "print(\"-\"*60)\n",
    "\n",
    "v = np.array([[4], [3]])\n",
    "v_proj = P @ v\n",
    "\n",
    "print(f\"\\nOriginal vector v: {v.ravel()}\")\n",
    "print(f\"Projected vector Pv: {v_proj.ravel()}\")\n",
    "\n",
    "# The projected vector should lie on the line\n",
    "# Check if v_proj is parallel to a\n",
    "# v_proj = scalar * a\n",
    "scalar = v_proj[0, 0] / a[0, 0]\n",
    "print(f\"\\nPv = {scalar:.4f} × a\")\n",
    "print(f\"Verify: {scalar:.4f} × {a.ravel()} = {(scalar * a).ravel()}\")\n",
    "print(f\"Match? {np.allclose(v_proj, scalar * a)} ✓\")\n",
    "\n",
    "# Perpendicular component\n",
    "v_perp = v - v_proj\n",
    "print(f\"\\nPerpendicular component: v - Pv = {v_perp.ravel()}\")\n",
    "\n",
    "# Check orthogonality: (v - Pv) ⊥ Pv\n",
    "dot_product = (v_perp.T @ v_proj)[0, 0]\n",
    "print(f\"\\nOrthogonality check:\")\n",
    "print(f\"(v - Pv) · Pv = {dot_product:.10f}\")\n",
    "print(f\"Is orthogonal? {np.isclose(dot_product, 0)} ✓\")\n",
    "\n",
    "# Projection onto a Plane\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Projection onto a Plane in ℝ³\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Define plane spanned by two vectors\n",
    "a1 = np.array([[1], [0], [0]])\n",
    "a2 = np.array([[0], [1], [0]])\n",
    "A = np.hstack([a1, a2])\n",
    "\n",
    "print(f\"\\nPlane spanned by:\")\n",
    "print(f\"a₁ = {a1.ravel()}\")\n",
    "print(f\"a₂ = {a2.ravel()}\")\n",
    "print(f\"\\nMatrix A = [a₁ a₂]:\")\n",
    "print(A)\n",
    "\n",
    "# Projection matrix: P = A(AᵀA)⁻¹Aᵀ\n",
    "ATA = A.T @ A\n",
    "ATA_inv = np.linalg.inv(ATA)\n",
    "P_plane = A @ ATA_inv @ A.T\n",
    "\n",
    "print(f\"\\nProjection matrix P:\")\n",
    "print(P_plane)\n",
    "\n",
    "# Project a vector onto the xy-plane\n",
    "v = np.array([[2], [3], [5]])\n",
    "v_proj = P_plane @ v\n",
    "\n",
    "print(f\"\\nVector v = {v.ravel()}\")\n",
    "print(f\"Projection Pv = {v_proj.ravel()}\")\n",
    "print(f\"\\nNote: z-component is removed (projected onto xy-plane) ✓\")\n",
    "\n",
    "# Verify properties\n",
    "print(f\"\\nVerify P² = P:\")\n",
    "print(f\"Equal? {np.allclose(P_plane @ P_plane, P_plane)} ✓\")\n",
    "\n",
    "# ML Application: Linear Regression\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ML Application: Linear Regression as Projection\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Generate synthetic data\n",
    "np.random.seed(42)\n",
    "n = 50\n",
    "X = np.random.randn(n, 2)  # 2 features\n",
    "true_weights = np.array([3, -2])\n",
    "y = X @ true_weights + np.random.randn(n) * 0.5\n",
    "\n",
    "print(f\"\\nData:\")\n",
    "print(f\"  X shape: {X.shape}\")\n",
    "print(f\"  y shape: {y.shape}\")\n",
    "print(f\"  True weights: {true_weights}\")\n",
    "\n",
    "# Linear regression: minimize ||y - Xβ||²\n",
    "# Solution: β = (XᵀX)⁻¹Xᵀy\n",
    "beta = np.linalg.inv(X.T @ X) @ X.T @ y\n",
    "\n",
    "print(f\"\\nEstimated weights β: {beta}\")\n",
    "print(f\"Close to true? {np.allclose(beta, true_weights, atol=0.1)} ✓\")\n",
    "\n",
    "# Projection matrix (hat matrix)\n",
    "P_hat = X @ np.linalg.inv(X.T @ X) @ X.T\n",
    "\n",
    "print(f\"\\nHat matrix P (projection matrix):\")\n",
    "print(f\"Shape: {P_hat.shape}\")\n",
    "\n",
    "# Predicted values\n",
    "y_pred = P_hat @ y  # Same as X @ beta\n",
    "\n",
    "print(f\"\\nPredictions:\")\n",
    "print(f\"ŷ = Py (projection of y onto column space of X)\")\n",
    "print(f\"First 5 predictions: {y_pred[:5]}\")\n",
    "print(f\"First 5 actual: {y[:5]}\")\n",
    "\n",
    "# Residuals\n",
    "residuals = y - y_pred\n",
    "print(f\"\\nResiduals: e = y - ŷ = (I - P)y\")\n",
    "print(f\"First 5 residuals: {residuals[:5]}\")\n",
    "\n",
    "# Verify orthogonality: Xᵀe = 0\n",
    "orthogonality_check = X.T @ residuals\n",
    "print(f\"\\nOrthogonality: Xᵀe should be ≈ 0\")\n",
    "print(f\"Xᵀe = {orthogonality_check}\")\n",
    "print(f\"All ≈ 0? {np.allclose(orthogonality_check, 0)} ✓\")\n",
    "\n",
    "print(f\"\\n→ Linear regression IS a projection!\")\n",
    "print(f\"→ Projects y onto space spanned by X columns\")\n",
    "print(f\"→ Residuals perpendicular to column space\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "orthogonality-header",
   "metadata": {},
   "source": [
    "## 25. Orthogonality & Orthogonal Matrices\n",
    "\n",
    "Orthogonal vectors are perpendicular (dot product = 0).\n",
    "\n",
    "**Orthogonal Matrix:** QᵀQ = QQᵀ = I (columns are orthonormal)\n",
    "\n",
    "**ML Application:** QR decomposition, rotations, whitening transformations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "orthogonality",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Orthogonal Vectors\n",
      "============================================================\n",
      "\n",
      "Standard basis vectors:\n",
      "e₁ = [1 0 0]\n",
      "e₂ = [0 1 0]\n",
      "e₃ = [0 0 1]\n",
      "\n",
      "Pairwise dot products:\n",
      "e₁ · e₂ = 0\n",
      "e₁ · e₃ = 0\n",
      "e₂ · e₃ = 0\n",
      "→ All zero: vectors are ORTHOGONAL ✓\n",
      "\n",
      "Lengths:\n",
      "||e₁|| = 1.0\n",
      "||e₂|| = 1.0\n",
      "||e₃|| = 1.0\n",
      "→ All length 1: vectors are ORTHONORMAL ✓\n",
      "\n",
      "============================================================\n",
      "Non-Orthogonal Vectors\n",
      "============================================================\n",
      "\n",
      "u = [1 2]\n",
      "v = [3 1]\n",
      "\n",
      "u · v = 5\n",
      "→ Not zero: vectors are NOT orthogonal ✗\n",
      "\n",
      "Angle between vectors: 45.00°\n",
      "\n",
      "------------------------------------------------------------\n",
      "Gram-Schmidt Orthogonalization\n",
      "------------------------------------------------------------\n",
      "\n",
      "Original:\n",
      "u = [1 2]\n",
      "v = [3 1]\n",
      "\n",
      "Orthogonalized:\n",
      "u_orth = [1 2]\n",
      "v_orth = [ 2. -1.]\n",
      "\n",
      "u_orth · v_orth = 0.0000000000\n",
      "Is orthogonal? True ✓\n",
      "\n",
      "Normalized (orthonormal):\n",
      "û = [0.4472 0.8944]\n",
      "v̂ = [ 0.8944 -0.4472]\n",
      "\n",
      "||û|| = 1.0000\n",
      "||v̂|| = 1.0000\n",
      "û · v̂ = 0.0000000000\n",
      "→ Orthonormal! ✓\n",
      "\n",
      "============================================================\n",
      "Orthogonal Matrix (Rotation)\n",
      "============================================================\n",
      "\n",
      "Rotation matrix Q (45°):\n",
      "[[ 0.7071 -0.7071]\n",
      " [ 0.7071  0.7071]]\n",
      "\n",
      "QᵀQ:\n",
      "[[ 1. -0.]\n",
      " [-0.  1.]]\n",
      "\n",
      "Identity I:\n",
      "[[1. 0.]\n",
      " [0. 1.]]\n",
      "\n",
      "QᵀQ = I? True ✓\n",
      "\n",
      "Q⁻¹:\n",
      "[[ 0.7071  0.7071]\n",
      " [-0.7071  0.7071]]\n",
      "\n",
      "Qᵀ:\n",
      "[[ 0.7071  0.7071]\n",
      " [-0.7071  0.7071]]\n",
      "\n",
      "Q⁻¹ = Qᵀ? True ✓\n",
      "\n",
      "------------------------------------------------------------\n",
      "Properties of Orthogonal Matrices\n",
      "------------------------------------------------------------\n",
      "\n",
      "1. Length preservation:\n",
      "   x = [3 4]\n",
      "   Qx = [-0.7071  4.9497]\n",
      "   ||x|| = 5.0000\n",
      "   ||Qx|| = 5.0000\n",
      "   Equal? True ✓\n",
      "\n",
      "2. Determinant:\n",
      "   det(Q) = 1.0000\n",
      "   |det(Q)| = 1? True ✓\n",
      "   → Rotation (det=1) preserves orientation\n",
      "\n",
      "3. Orthonormal columns:\n",
      "   Column 1: [0.7071 0.7071]\n",
      "   Column 2: [-0.7071  0.7071]\n",
      "   col1 · col2 = 0.0000000000 (≈ 0) ✓\n",
      "   ||col1|| = 1.0000 (= 1) ✓\n",
      "   ||col2|| = 1.0000 (= 1) ✓\n",
      "\n",
      "============================================================\n",
      "ML Application: Whitening (Decorrelation)\n",
      "============================================================\n",
      "\n",
      "Original data covariance:\n",
      "[[1.9355 1.3249]\n",
      " [1.3249 1.0256]]\n",
      "→ Features are correlated (off-diagonal ≠ 0)\n",
      "\n",
      "Whitened data covariance:\n",
      "[[1. 0.]\n",
      " [0. 1.]]\n",
      "→ Identity matrix! Features decorrelated ✓\n",
      "\n",
      "Verify it's identity:\n",
      "Close to I? True ✓\n",
      "\n",
      "→ Whitening uses orthogonal transformations\n",
      "→ Decorrelates features\n",
      "→ Used in preprocessing for ML models\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Aamir Bin Abbas\\AppData\\Local\\Temp\\ipykernel_2092\\2295758346.py:193: RuntimeWarning: covariance is not symmetric positive-semidefinite.\n",
      "  X = np.random.multivariate_normal(mean, cov, n_samples)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Orthogonality and Orthogonal Matrices\n",
    "\n",
    "ORTHOGONAL VECTORS:\n",
    "- u ⊥ v if u · v = 0\n",
    "- Perpendicular, at 90° angle\n",
    "\n",
    "ORTHONORMAL VECTORS:\n",
    "- Orthogonal AND unit length\n",
    "- u · v = 0 (different) and ||u|| = ||v|| = 1\n",
    "\n",
    "ORTHOGONAL MATRIX Q:\n",
    "- Columns are orthonormal\n",
    "- QᵀQ = I\n",
    "- Q⁻¹ = Qᵀ (inverse = transpose!)\n",
    "- Preserves lengths and angles\n",
    "- Represents rotation/reflection\n",
    "\n",
    "Properties:\n",
    "- ||Qx|| = ||x|| (preserves length)\n",
    "- (Qx) · (Qy) = x · y (preserves dot products)\n",
    "- det(Q) = ±1\n",
    "\"\"\"\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"Orthogonal Vectors\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Example 1: Standard basis (orthogonal)\n",
    "e1 = np.array([1, 0, 0])\n",
    "e2 = np.array([0, 1, 0])\n",
    "e3 = np.array([0, 0, 1])\n",
    "\n",
    "print(f\"\\nStandard basis vectors:\")\n",
    "print(f\"e₁ = {e1}\")\n",
    "print(f\"e₂ = {e2}\")\n",
    "print(f\"e₃ = {e3}\")\n",
    "\n",
    "# Check pairwise orthogonality\n",
    "print(f\"\\nPairwise dot products:\")\n",
    "print(f\"e₁ · e₂ = {np.dot(e1, e2)}\")\n",
    "print(f\"e₁ · e₃ = {np.dot(e1, e3)}\")\n",
    "print(f\"e₂ · e₃ = {np.dot(e2, e3)}\")\n",
    "print(f\"→ All zero: vectors are ORTHOGONAL ✓\")\n",
    "\n",
    "# Check unit length\n",
    "print(f\"\\nLengths:\")\n",
    "print(f\"||e₁|| = {np.linalg.norm(e1)}\")\n",
    "print(f\"||e₂|| = {np.linalg.norm(e2)}\")\n",
    "print(f\"||e₃|| = {np.linalg.norm(e3)}\")\n",
    "print(f\"→ All length 1: vectors are ORTHONORMAL ✓\")\n",
    "\n",
    "# Example 2: Non-orthogonal vectors\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Non-Orthogonal Vectors\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "u = np.array([1, 2])\n",
    "v = np.array([3, 1])\n",
    "\n",
    "dot_uv = np.dot(u, v)\n",
    "\n",
    "print(f\"\\nu = {u}\")\n",
    "print(f\"v = {v}\")\n",
    "print(f\"\\nu · v = {dot_uv}\")\n",
    "print(f\"→ Not zero: vectors are NOT orthogonal ✗\")\n",
    "\n",
    "# Compute angle\n",
    "cos_theta = dot_uv / (np.linalg.norm(u) * np.linalg.norm(v))\n",
    "theta_deg = np.degrees(np.arccos(cos_theta))\n",
    "print(f\"\\nAngle between vectors: {theta_deg:.2f}°\")\n",
    "\n",
    "# Gram-Schmidt: Make them orthogonal\n",
    "print(\"\\n\" + \"-\"*60)\n",
    "print(\"Gram-Schmidt Orthogonalization\")\n",
    "print(\"-\"*60)\n",
    "\n",
    "# Keep u as is\n",
    "u_orth = u\n",
    "\n",
    "# Make v orthogonal to u\n",
    "# v_orth = v - projection of v onto u\n",
    "projection = (np.dot(v, u) / np.dot(u, u)) * u\n",
    "v_orth = v - projection\n",
    "\n",
    "print(f\"\\nOriginal:\")\n",
    "print(f\"u = {u}\")\n",
    "print(f\"v = {v}\")\n",
    "\n",
    "print(f\"\\nOrthogonalized:\")\n",
    "print(f\"u_orth = {u_orth}\")\n",
    "print(f\"v_orth = {v_orth}\")\n",
    "\n",
    "# Verify orthogonality\n",
    "dot_orth = np.dot(u_orth, v_orth)\n",
    "print(f\"\\nu_orth · v_orth = {dot_orth:.10f}\")\n",
    "print(f\"Is orthogonal? {np.isclose(dot_orth, 0)} ✓\")\n",
    "\n",
    "# Normalize to make orthonormal\n",
    "u_orthonormal = u_orth / np.linalg.norm(u_orth)\n",
    "v_orthonormal = v_orth / np.linalg.norm(v_orth)\n",
    "\n",
    "print(f\"\\nNormalized (orthonormal):\")\n",
    "print(f\"û = {u_orthonormal}\")\n",
    "print(f\"v̂ = {v_orthonormal}\")\n",
    "print(f\"\\n||û|| = {np.linalg.norm(u_orthonormal):.4f}\")\n",
    "print(f\"||v̂|| = {np.linalg.norm(v_orthonormal):.4f}\")\n",
    "print(f\"û · v̂ = {np.dot(u_orthonormal, v_orthonormal):.10f}\")\n",
    "print(f\"→ Orthonormal! ✓\")\n",
    "\n",
    "# Orthogonal Matrices\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Orthogonal Matrix (Rotation)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Rotation matrix (45° in 2D)\n",
    "theta = np.pi / 4  # 45 degrees\n",
    "Q = np.array([\n",
    "    [np.cos(theta), -np.sin(theta)],\n",
    "    [np.sin(theta),  np.cos(theta)]\n",
    "])\n",
    "\n",
    "print(f\"\\nRotation matrix Q (45°):\")\n",
    "print(Q)\n",
    "\n",
    "# Verify QᵀQ = I\n",
    "QTQ = Q.T @ Q\n",
    "I = np.eye(2)\n",
    "\n",
    "print(f\"\\nQᵀQ:\")\n",
    "print(QTQ)\n",
    "print(f\"\\nIdentity I:\")\n",
    "print(I)\n",
    "print(f\"\\nQᵀQ = I? {np.allclose(QTQ, I)} ✓\")\n",
    "\n",
    "# Verify Q⁻¹ = Qᵀ\n",
    "Q_inv = np.linalg.inv(Q)\n",
    "print(f\"\\nQ⁻¹:\")\n",
    "print(Q_inv)\n",
    "print(f\"\\nQᵀ:\")\n",
    "print(Q.T)\n",
    "print(f\"\\nQ⁻¹ = Qᵀ? {np.allclose(Q_inv, Q.T)} ✓\")\n",
    "\n",
    "# Properties\n",
    "print(\"\\n\" + \"-\"*60)\n",
    "print(\"Properties of Orthogonal Matrices\")\n",
    "print(\"-\"*60)\n",
    "\n",
    "# Property 1: Preserves length\n",
    "x = np.array([3, 4])\n",
    "Qx = Q @ x\n",
    "\n",
    "norm_x = np.linalg.norm(x)\n",
    "norm_Qx = np.linalg.norm(Qx)\n",
    "\n",
    "print(f\"\\n1. Length preservation:\")\n",
    "print(f\"   x = {x}\")\n",
    "print(f\"   Qx = {Qx}\")\n",
    "print(f\"   ||x|| = {norm_x:.4f}\")\n",
    "print(f\"   ||Qx|| = {norm_Qx:.4f}\")\n",
    "print(f\"   Equal? {np.isclose(norm_x, norm_Qx)} ✓\")\n",
    "\n",
    "# Property 2: Determinant = ±1\n",
    "det_Q = np.linalg.det(Q)\n",
    "print(f\"\\n2. Determinant:\")\n",
    "print(f\"   det(Q) = {det_Q:.4f}\")\n",
    "print(f\"   |det(Q)| = 1? {np.isclose(abs(det_Q), 1)} ✓\")\n",
    "print(f\"   → Rotation (det=1) preserves orientation\")\n",
    "\n",
    "# Property 3: Orthonormal columns\n",
    "col1 = Q[:, 0]\n",
    "col2 = Q[:, 1]\n",
    "\n",
    "print(f\"\\n3. Orthonormal columns:\")\n",
    "print(f\"   Column 1: {col1}\")\n",
    "print(f\"   Column 2: {col2}\")\n",
    "print(f\"   col1 · col2 = {np.dot(col1, col2):.10f} (≈ 0) ✓\")\n",
    "print(f\"   ||col1|| = {np.linalg.norm(col1):.4f} (= 1) ✓\")\n",
    "print(f\"   ||col2|| = {np.linalg.norm(col2):.4f} (= 1) ✓\")\n",
    "\n",
    "# ML Application: Whitening Transform\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ML Application: Whitening (Decorrelation)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Generate correlated data\n",
    "np.random.seed(42)\n",
    "n_samples = 500\n",
    "mean = [0, 0]\n",
    "cov = [[2, 1.5],   # Correlated\n",
    "       [1.5, 1]]\n",
    "\n",
    "X = np.random.multivariate_normal(mean, cov, n_samples)\n",
    "\n",
    "print(f\"\\nOriginal data covariance:\")\n",
    "cov_original = np.cov(X.T)\n",
    "print(cov_original)\n",
    "print(f\"→ Features are correlated (off-diagonal ≠ 0)\")\n",
    "\n",
    "# Whiten using eigendecomposition\n",
    "# Cov = QΛQᵀ\n",
    "eigenvalues, Q = np.linalg.eigh(cov_original)\n",
    "Lambda = np.diag(eigenvalues)\n",
    "\n",
    "# Whitening matrix: W = Λ^(-1/2) Qᵀ\n",
    "Lambda_inv_sqrt = np.diag(1.0 / np.sqrt(eigenvalues))\n",
    "W = Lambda_inv_sqrt @ Q.T\n",
    "\n",
    "# Whiten the data\n",
    "X_white = (W @ X.T).T\n",
    "\n",
    "print(f\"\\nWhitened data covariance:\")\n",
    "cov_white = np.cov(X_white.T)\n",
    "print(cov_white)\n",
    "print(f\"→ Identity matrix! Features decorrelated ✓\")\n",
    "\n",
    "print(f\"\\nVerify it's identity:\")\n",
    "print(f\"Close to I? {np.allclose(cov_white, np.eye(2), atol=0.1)} ✓\")\n",
    "\n",
    "print(f\"\\n→ Whitening uses orthogonal transformations\")\n",
    "print(f\"→ Decorrelates features\")\n",
    "print(f\"→ Used in preprocessing for ML models\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eigenvalues-header",
   "metadata": {},
   "source": [
    "## 26. Eigenvalues & Eigenvectors\n",
    "\n",
    "For square matrix A, if Av = λv (v ≠ 0), then:\n",
    "- v is an **eigenvector**\n",
    "- λ is an **eigenvalue**\n",
    "\n",
    "**Interpretation:** Direction that only gets scaled, not rotated.\n",
    "\n",
    "**ML Application:** PCA, spectral clustering, PageRank, stability analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eigenvalues",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Eigenvalues and Eigenvectors\n",
    "\n",
    "Definition:\n",
    "For square matrix A, a non-zero vector v is an eigenvector if:\n",
    "    Av = λv\n",
    "\n",
    "where λ is a scalar (the eigenvalue)\n",
    "\n",
    "Interpretation:\n",
    "- Av is parallel to v (just scaled)\n",
    "- Direction v is preserved under transformation A\n",
    "- λ tells how much v is stretched/compressed\n",
    "  - |λ| > 1: stretches\n",
    "  - |λ| < 1: compresses  \n",
    "  - λ < 0: flips direction\n",
    "\n",
    "Computing:\n",
    "1. Solve det(A - λI) = 0 for eigenvalues λ\n",
    "2. For each λ, solve (A - λI)v = 0 for eigenvector v\n",
    "\n",
    "ML Applications:\n",
    "- PCA: eigenvectors = principal components\n",
    "- Eigenvalues = variance along components\n",
    "- Spectral clustering\n",
    "- Stability of dynamical systems (RNNs)\n",
    "\"\"\"\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"Eigenvalues and Eigenvectors\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Simple 2×2 example\n",
    "A = np.array([\n",
    "    [4, 2],\n",
    "    [1, 3]\n",
    "])\n",
    "\n",
    "print(f\"\\nMatrix A:\")\n",
    "print(A)\n",
    "\n",
    "# Compute eigenvalues and eigenvectors\n",
    "eigenvalues, eigenvectors = np.linalg.eig(A)\n",
    "\n",
    "print(f\"\\nEigenvalues λ:\")\n",
    "print(eigenvalues)\n",
    "\n",
    "print(f\"\\nEigenvectors (as columns):\")\n",
    "print(eigenvectors)\n",
    "\n",
    "# Verify for each eigenvalue/eigenvector pair\n",
    "print(\"\\n\" + \"-\"*60)\n",
    "print(\"Verification: Av = λv\")\n",
    "print(\"-\"*60)\n",
    "\n",
    "for i in range(len(eigenvalues)):\n",
    "    λ = eigenvalues[i]\n",
    "    v = eigenvectors[:, i]\n",
    "    \n",
    "    Av = A @ v\n",
    "    λv = λ * v\n",
    "    \n",
    "    print(f\"\\nEigenpair {i+1}:\")\n",
    "    print(f\"  λ = {λ:.4f}\")\n",
    "    print(f\"  v = {v}\")\n",
    "    print(f\"  Av = {Av}\")\n",
    "    print(f\"  λv = {λv}\")\n",
    "    print(f\"  Av = λv? {np.allclose(Av, λv)} ✓\")\n",
    "\n",
    "# Geometric interpretation\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Geometric Interpretation\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"\\nEigenvector 1 direction: {eigenvectors[:, 0]}\")\n",
    "print(f\"Eigenvalue 1: {eigenvalues[0]:.4f}\")\n",
    "if abs(eigenvalues[0]) > 1:\n",
    "    print(f\"→ Vector gets STRETCHED by factor {abs(eigenvalues[0]):.4f}\")\n",
    "else:\n",
    "    print(f\"→ Vector gets COMPRESSED by factor {abs(eigenvalues[0]):.4f}\")\n",
    "\n",
    "print(f\"\\nEigenvector 2 direction: {eigenvectors[:, 1]}\")\n",
    "print(f\"Eigenvalue 2: {eigenvalues[1]:.4f}\")\n",
    "if abs(eigenvalues[1]) > 1:\n",
    "    print(f\"→ Vector gets STRETCHED by factor {abs(eigenvalues[1]):.4f}\")\n",
    "else:\n",
    "    print(f\"→ Vector gets COMPRESSED by factor {abs(eigenvalues[1]):.4f}\")\n",
    "\n",
    "# Eigendecomposition\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Eigendecomposition: A = QΛQ⁻¹\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "Q = eigenvectors  # Matrix of eigenvectors\n",
    "Lambda = np.diag(eigenvalues)  # Diagonal matrix of eigenvalues\n",
    "\n",
    "print(f\"\\nQ (eigenvectors as columns):\")\n",
    "print(Q)\n",
    "\n",
    "print(f\"\\nΛ (eigenvalues on diagonal):\")\n",
    "print(Lambda)\n",
    "\n",
    "# Reconstruct A\n",
    "Q_inv = np.linalg.inv(Q)\n",
    "A_reconstructed = Q @ Lambda @ Q_inv\n",
    "\n",
    "print(f\"\\nReconstructed A = QΛQ⁻¹:\")\n",
    "print(A_reconstructed)\n",
    "\n",
    "print(f\"\\nOriginal A:\")\n",
    "print(A)\n",
    "\n",
    "print(f\"\\nMatch? {np.allclose(A_reconstructed, A)} ✓\")\n",
    "\n",
    "# Power of eigendecomposition: Easy to compute A^n\n",
    "print(\"\\n\" + \"-\"*60)\n",
    "print(\"Computing Matrix Powers: A^n = QΛ^nQ⁻¹\")\n",
    "print(\"-\"*60)\n",
    "\n",
    "n = 10\n",
    "\n",
    "# Method 1: Direct (slow for large n)\n",
    "A_power_direct = np.linalg.matrix_power(A, n)\n",
    "\n",
    "# Method 2: Using eigendecomposition (fast!)\n",
    "Lambda_n = np.diag(eigenvalues**n)  # Just raise eigenvalues to power n\n",
    "A_power_eigen = Q @ Lambda_n @ Q_inv\n",
    "\n",
    "print(f\"\\nA^{n} (direct):\")\n",
    "print(A_power_direct)\n",
    "\n",
    "print(f\"\\nA^{n} (eigendecomposition):\")\n",
    "print(A_power_eigen)\n",
    "\n",
    "print(f\"\\nMatch? {np.allclose(A_power_direct, A_power_eigen)} ✓\")\n",
    "\n",
    "print(f\"\\n→ Eigendecomposition makes powers easy!\")\n",
    "print(f\"→ Just raise diagonal eigenvalues to power\")\n",
    "\n",
    "# Special matrices\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Symmetric Matrices: Special Properties\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Symmetric matrix (like covariance)\n",
    "S = np.array([\n",
    "    [4, 1],\n",
    "    [1, 3]\n",
    "])\n",
    "\n",
    "print(f\"\\nSymmetric matrix S:\")\n",
    "print(S)\n",
    "print(f\"S = Sᵀ? {np.allclose(S, S.T)} ✓\")\n",
    "\n",
    "eigenvalues_sym, eigenvectors_sym = np.linalg.eigh(S)  # Use eigh for symmetric\n",
    "\n",
    "print(f\"\\nEigenvalues:\")\n",
    "print(eigenvalues_sym)\n",
    "print(f\"→ All REAL (guaranteed for symmetric) ✓\")\n",
    "\n",
    "# Check orthogonality of eigenvectors\n",
    "v1 = eigenvectors_sym[:, 0]\n",
    "v2 = eigenvectors_sym[:, 1]\n",
    "dot_v1_v2 = np.dot(v1, v2)\n",
    "\n",
    "print(f\"\\nEigenvectors:\")\n",
    "print(f\"v₁ = {v1}\")\n",
    "print(f\"v₂ = {v2}\")\n",
    "print(f\"v₁ · v₂ = {dot_v1_v2:.10f}\")\n",
    "print(f\"→ ORTHOGONAL (guaranteed for symmetric) ✓\")\n",
    "\n",
    "print(f\"\\nFor symmetric matrices:\")\n",
    "print(f\"1. Eigenvalues are real\")\n",
    "print(f\"2. Eigenvectors are orthogonal\")\n",
    "print(f\"3. Can write S = QΛQᵀ (Q is orthogonal!)\")\n",
    "\n",
    "# ML Application: PCA\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ML Application: PCA via Eigendecomposition\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Generate data\n",
    "np.random.seed(42)\n",
    "n_samples = 200\n",
    "mean = [0, 0]\n",
    "cov = [[3, 1.5],\n",
    "       [1.5, 1]]\n",
    "\n",
    "X = np.random.multivariate_normal(mean, cov, n_samples)\n",
    "\n",
    "print(f\"\\nData shape: {X.shape}\")\n",
    "\n",
    "# Center the data\n",
    "X_centered = X - X.mean(axis=0)\n",
    "\n",
    "# Compute covariance matrix\n",
    "C = np.cov(X_centered.T)\n",
    "\n",
    "print(f\"\\nCovariance matrix:\")\n",
    "print(C)\n",
    "\n",
    "# Eigendecomposition of covariance\n",
    "eigenvalues_pca, eigenvectors_pca = np.linalg.eigh(C)\n",
    "\n",
    "# Sort by eigenvalue (descending)\n",
    "idx = eigenvalues_pca.argsort()[::-1]\n",
    "eigenvalues_pca = eigenvalues_pca[idx]\n",
    "eigenvectors_pca = eigenvectors_pca[:, idx]\n",
    "\n",
    "print(f\"\\nPrincipal Components (eigenvectors):\")\n",
    "print(f\"PC1: {eigenvectors_pca[:, 0]}\")\n",
    "print(f\"PC2: {eigenvectors_pca[:, 1]}\")\n",
    "\n",
    "print(f\"\\nVariance explained (eigenvalues):\")\n",
    "print(f\"PC1: {eigenvalues_pca[0]:.4f}\")\n",
    "print(f\"PC2: {eigenvalues_pca[1]:.4f}\")\n",
    "\n",
    "# Variance explained ratio\n",
    "total_variance = eigenvalues_pca.sum()\n",
    "explained_variance_ratio = eigenvalues_pca / total_variance\n",
    "\n",
    "print(f\"\\nVariance explained ratio:\")\n",
    "print(f\"PC1: {explained_variance_ratio[0]*100:.2f}%\")\n",
    "print(f\"PC2: {explained_variance_ratio[1]*100:.2f}%\")\n",
    "\n",
    "# Project data onto principal components\n",
    "X_pca = X_centered @ eigenvectors_pca\n",
    "\n",
    "print(f\"\\nTransformed data shape: {X_pca.shape}\")\n",
    "\n",
    "# Verify: variance in PC directions\n",
    "var_pc1 = np.var(X_pca[:, 0])\n",
    "var_pc2 = np.var(X_pca[:, 1])\n",
    "\n",
    "print(f\"\\nVerification:\")\n",
    "print(f\"Variance in PC1 direction: {var_pc1:.4f}\")\n",
    "print(f\"Eigenvalue λ₁: {eigenvalues_pca[0]:.4f}\")\n",
    "print(f\"Match? {np.isclose(var_pc1, eigenvalues_pca[0])} ✓\")\n",
    "\n",
    "print(f\"\\n→ PCA finds directions of maximum variance\")\n",
    "print(f\"→ These directions are eigenvectors of covariance\")\n",
    "print(f\"→ Variance along each = corresponding eigenvalue\")\n",
    "print(f\"→ Dimensionality reduction: keep top k components\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cbdf622",
   "metadata": {},
   "source": [
    "# Linear Algebra for AI/ML - Part 4: Summary Table\n",
    "\n",
    "| Concept | Key Formula(s) | Properties | ML Application |\n",
    "|---------|----------------|------------|----------------|\n",
    "| Projection Matrix | P = A(AᵀA)⁻¹Aᵀ | Idempotent (P² = P), Symmetric (Pᵀ = P) | Linear regression (ŷ = Py), PCA, feature projection |\n",
    "| Orthogonal Vectors | u · v = 0 | Perpendicular, length preserved under orthogonal transforms | Decorrelation, whitening, Gram–Schmidt orthogonalization |\n",
    "| Orthogonal Matrix | QᵀQ = I, Q⁻¹ = Qᵀ | Columns orthonormal, preserves length & angles, det(Q) = ±1 | Rotations, QR decomposition, whitening transforms |\n",
    "| Eigenvalues & Eigenvectors | Av = λv | Eigenvectors invariant under transformation, eigenvalues scale | PCA, spectral clustering, stability analysis, matrix powers |\n",
    "| Eigendecomposition | A = QΛQ⁻¹ | For symmetric matrices: eigenvalues real, eigenvectors orthogonal | Principal Component Analysis (PCA), variance analysis |\n",
    "| PCA via Covariance | C = XᵀX/(n-1) | Eigenvectors = principal components, eigenvalues = variance | Dimensionality reduction, feature extraction, data visualization |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "continue-part4",
   "metadata": {},
   "source": [
    "## Part 4 Complete!\n",
    "\n",
    "**Completed:**\n",
    "- Projection Matrices (24)\n",
    "- Orthogonality & Orthogonal Matrices (25)\n",
    "- Eigenvalues & Eigenvectors (26)\n",
    "\n",
    "**Part 5 will cover:**\n",
    "- SVD (Singular Value Decomposition)\n",
    "- Matrix Decompositions (Cholesky, QR, LU)\n",
    "- Trace & Determinant properties\n",
    "- Covariance Matrix.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
