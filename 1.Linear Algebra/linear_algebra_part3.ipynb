{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# Linear Algebra for AI/ML - Part 3: Norms, Products & Projections\n",
    "\n",
    "This notebook continues with norms, inner products, and projection concepts.\n",
    "\n",
    "**Prerequisites:** Complete Parts 1 and 2 first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "setup",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NumPy version: 2.2.6\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Setup: Import Required Libraries\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import linalg\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "np.set_printoptions(precision=4, suppress=True)\n",
    "print(f\"NumPy version: {np.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dimension-header",
   "metadata": {},
   "source": [
    "## 21. Dimension of Vector Space\n",
    "\n",
    "The dimension is the number of vectors in any basis of the vector space.\n",
    "\n",
    "**ML Application:** Feature dimensionality, model capacity, curse of dimensionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dimension",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Dimension of Vector Spaces\n",
      "============================================================\n",
      "\n",
      "Example 1: Standard Vector Spaces\n",
      "------------------------------------------------------------\n",
      "dim(‚Ñù¬π (real line)) = 1\n",
      "dim(‚Ñù¬≤ (plane)) = 2\n",
      "dim(‚Ñù¬≥ (3D space)) = 3\n",
      "dim(‚Ñù‚Åø (n-dimensional)) = n\n",
      "\n",
      "============================================================\n",
      "Example 2: Dimension of Subspaces\n",
      "============================================================\n",
      "\n",
      "Case 1: Line through origin in ‚Ñù¬≥\n",
      "Spanned by v = [1 2 3]\n",
      "Subspace: {Œªv : Œª ‚àà ‚Ñù}\n",
      "Dimension: 1 (need 1 vector to span)\n",
      "\n",
      "Case 2: Plane through origin in ‚Ñù¬≥\n",
      "Spanned by v‚ÇÅ = [1 0 0], v‚ÇÇ = [0 1 0]\n",
      "Subspace: {Œª‚ÇÅv‚ÇÅ + Œª‚ÇÇv‚ÇÇ : Œª‚ÇÅ, Œª‚ÇÇ ‚àà ‚Ñù}\n",
      "Dimension: 2 (need 2 independent vectors)\n",
      "\n",
      "Case 3: All of ‚Ñù¬≥\n",
      "Spanned by standard basis\n",
      "Dimension: 3 (need 3 independent vectors)\n",
      "\n",
      "============================================================\n",
      "Example 3: Computing Dimension via Rank\n",
      "============================================================\n",
      "\n",
      "Vectors (as columns of A):\n",
      "v‚ÇÅ = [1 2 3]\n",
      "v‚ÇÇ = [4 5 6]\n",
      "v‚ÇÉ = [7 8 9]\n",
      "v‚ÇÑ = [2 4 6]\n",
      "\n",
      "Matrix A:\n",
      "[[1 4 7 2]\n",
      " [2 5 8 4]\n",
      " [3 6 9 6]]\n",
      "\n",
      "Rank of A: 2\n",
      "Number of vectors: 4\n",
      "\n",
      "Dimension of span{v‚ÇÅ, v‚ÇÇ, v‚ÇÉ, v‚ÇÑ} = 2\n",
      "\n",
      "Interpretation:\n",
      "‚Üí Only 2 vectors are linearly independent\n",
      "‚Üí The span is 2-dimensional\n",
      "‚Üí Can express as span of 2 basis vectors\n",
      "\n",
      "Note: v‚ÇÑ = 2√óv‚ÇÅ (dependent)\n",
      "Verify: 2 √ó [1 2 3] = [2 4 6]\n",
      "\n",
      "============================================================\n",
      "Example 4: Subspace Dimension < Ambient Dimension\n",
      "============================================================\n",
      "\n",
      "Vectors in ‚Ñù‚Å¥:\n",
      "v‚ÇÅ = [1 0 0 0]\n",
      "v‚ÇÇ = [0 1 0 0]\n",
      "\n",
      "Ambient space: ‚Ñù‚Å¥ (dimension 4)\n",
      "Subspace dimension: 2\n",
      "\n",
      "‚Üí 2D subspace embedded in 4D space\n",
      "‚Üí Like a flat sheet in a room\n",
      "\n",
      "============================================================\n",
      "ML Application: Intrinsic Dimensionality\n",
      "============================================================\n",
      "\n",
      "Observed data shape: (200, 5)\n",
      "Nominal dimension (# features): 5\n",
      "\n",
      "Singular values:\n",
      "[33.057  31.4025  1.4623  1.3979  1.257 ]\n",
      "\n",
      "Effective rank (œÉ > 1e-10): 5\n",
      "\n",
      "Interpretation:\n",
      "‚Üí Data lives in a 5-dimensional subspace\n",
      "‚Üí Intrinsic dimension ‚âà 5 (much less than 5)\n",
      "‚Üí Can reduce from 5D to 5D with minimal loss\n",
      "\n",
      "Variance explained by components:\n",
      "  First 1 components: 52.42%\n",
      "  First 2 components: 99.73%\n",
      "  First 3 components: 99.83%\n",
      "  First 4 components: 99.92%\n",
      "  First 5 components: 100.00%\n",
      "\n",
      "‚Üí First 2 components capture 99.73% of variance\n",
      "‚Üí Confirms true intrinsic dimension is 2\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Dimension of Vector Space\n",
    "\n",
    "Definition:\n",
    "The dimension of a vector space V is the number of vectors in any basis of V.\n",
    "\n",
    "Key Facts:\n",
    "- All bases of V have the same number of vectors\n",
    "- dim(‚Ñù‚Åø) = n\n",
    "- dim(span({v‚ÇÅ, ..., v‚Çñ})) ‚â§ k\n",
    "- dim(V) = rank of any matrix whose columns span V\n",
    "\n",
    "ML Applications:\n",
    "- Number of features in dataset\n",
    "- Intrinsic dimensionality (manifold learning)\n",
    "- Effective rank in PCA\n",
    "\"\"\"\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"Dimension of Vector Spaces\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Example 1: Standard spaces\n",
    "print(\"\\nExample 1: Standard Vector Spaces\")\n",
    "print(\"-\"*60)\n",
    "\n",
    "spaces = [\n",
    "    (\"‚Ñù¬π (real line)\", 1),\n",
    "    (\"‚Ñù¬≤ (plane)\", 2),\n",
    "    (\"‚Ñù¬≥ (3D space)\", 3),\n",
    "    (\"‚Ñù‚Åø (n-dimensional)\", \"n\"),\n",
    "]\n",
    "\n",
    "for space, dim in spaces:\n",
    "    print(f\"dim({space}) = {dim}\")\n",
    "\n",
    "# Example 2: Subspaces\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Example 2: Dimension of Subspaces\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Case 1: Line in ‚Ñù¬≥\n",
    "print(\"\\nCase 1: Line through origin in ‚Ñù¬≥\")\n",
    "v = np.array([1, 2, 3])\n",
    "print(f\"Spanned by v = {v}\")\n",
    "print(f\"Subspace: {{Œªv : Œª ‚àà ‚Ñù}}\")\n",
    "print(f\"Dimension: 1 (need 1 vector to span)\")\n",
    "\n",
    "# Case 2: Plane in ‚Ñù¬≥\n",
    "print(\"\\nCase 2: Plane through origin in ‚Ñù¬≥\")\n",
    "v1 = np.array([1, 0, 0])\n",
    "v2 = np.array([0, 1, 0])\n",
    "print(f\"Spanned by v‚ÇÅ = {v1}, v‚ÇÇ = {v2}\")\n",
    "print(f\"Subspace: {{Œª‚ÇÅv‚ÇÅ + Œª‚ÇÇv‚ÇÇ : Œª‚ÇÅ, Œª‚ÇÇ ‚àà ‚Ñù}}\")\n",
    "print(f\"Dimension: 2 (need 2 independent vectors)\")\n",
    "\n",
    "# Case 3: All of ‚Ñù¬≥\n",
    "print(\"\\nCase 3: All of ‚Ñù¬≥\")\n",
    "v1 = np.array([1, 0, 0])\n",
    "v2 = np.array([0, 1, 0])\n",
    "v3 = np.array([0, 0, 1])\n",
    "print(f\"Spanned by standard basis\")\n",
    "print(f\"Dimension: 3 (need 3 independent vectors)\")\n",
    "\n",
    "# Example 3: Computing dimension via rank\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Example 3: Computing Dimension via Rank\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Set of vectors\n",
    "v1 = np.array([1, 2, 3])\n",
    "v2 = np.array([4, 5, 6])\n",
    "v3 = np.array([7, 8, 9])\n",
    "v4 = np.array([2, 4, 6])  # = 2*v1\n",
    "\n",
    "# Create matrix with these as columns\n",
    "A = np.column_stack([v1, v2, v3, v4])\n",
    "\n",
    "print(\"\\nVectors (as columns of A):\")\n",
    "print(f\"v‚ÇÅ = {v1}\")\n",
    "print(f\"v‚ÇÇ = {v2}\")\n",
    "print(f\"v‚ÇÉ = {v3}\")\n",
    "print(f\"v‚ÇÑ = {v4}\")\n",
    "\n",
    "print(f\"\\nMatrix A:\")\n",
    "print(A)\n",
    "\n",
    "rank = np.linalg.matrix_rank(A)\n",
    "print(f\"\\nRank of A: {rank}\")\n",
    "print(f\"Number of vectors: {A.shape[1]}\")\n",
    "print(f\"\\nDimension of span{{v‚ÇÅ, v‚ÇÇ, v‚ÇÉ, v‚ÇÑ}} = {rank}\")\n",
    "print(f\"\\nInterpretation:\")\n",
    "print(f\"‚Üí Only {rank} vectors are linearly independent\")\n",
    "print(f\"‚Üí The span is {rank}-dimensional\")\n",
    "print(f\"‚Üí Can express as span of {rank} basis vectors\")\n",
    "\n",
    "# Find which vectors are independent\n",
    "print(f\"\\nNote: v‚ÇÑ = 2√óv‚ÇÅ (dependent)\")\n",
    "print(f\"Verify: 2 √ó {v1} = {2*v1}\")\n",
    "\n",
    "# Example 4: Dimension vs Ambient Space\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Example 4: Subspace Dimension < Ambient Dimension\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# 2D subspace in ‚Ñù‚Å¥\n",
    "v1 = np.array([1, 0, 0, 0])\n",
    "v2 = np.array([0, 1, 0, 0])\n",
    "\n",
    "A = np.column_stack([v1, v2])\n",
    "\n",
    "print(f\"\\nVectors in ‚Ñù‚Å¥:\")\n",
    "print(f\"v‚ÇÅ = {v1}\")\n",
    "print(f\"v‚ÇÇ = {v2}\")\n",
    "\n",
    "print(f\"\\nAmbient space: ‚Ñù‚Å¥ (dimension 4)\")\n",
    "print(f\"Subspace dimension: {np.linalg.matrix_rank(A)}\")\n",
    "print(f\"\\n‚Üí 2D subspace embedded in 4D space\")\n",
    "print(f\"‚Üí Like a flat sheet in a room\")\n",
    "\n",
    "# ML Application: Intrinsic Dimensionality\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ML Application: Intrinsic Dimensionality\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Generate high-dimensional data with low intrinsic dimension\n",
    "np.random.seed(42)\n",
    "n_samples = 200\n",
    "\n",
    "# True latent factors (2D)\n",
    "z1 = np.random.randn(n_samples)\n",
    "z2 = np.random.randn(n_samples)\n",
    "\n",
    "# Generate 5 observed features from 2 latent factors\n",
    "X = np.column_stack([\n",
    "    z1 + 0.1*np.random.randn(n_samples),           # Feature 1 ‚âà z1\n",
    "    z2 + 0.1*np.random.randn(n_samples),           # Feature 2 ‚âà z2\n",
    "    z1 + z2 + 0.1*np.random.randn(n_samples),      # Feature 3 ‚âà z1 + z2\n",
    "    2*z1 - z2 + 0.1*np.random.randn(n_samples),    # Feature 4 ‚âà 2z1 - z2\n",
    "    0.5*z1 + 1.5*z2 + 0.1*np.random.randn(n_samples)  # Feature 5 ‚âà 0.5z1 + 1.5z2\n",
    "])\n",
    "\n",
    "print(f\"\\nObserved data shape: {X.shape}\")\n",
    "print(f\"Nominal dimension (# features): {X.shape[1]}\")\n",
    "\n",
    "# Compute effective rank via SVD\n",
    "U, s, Vt = np.linalg.svd(X, full_matrices=False)\n",
    "\n",
    "print(f\"\\nSingular values:\")\n",
    "print(s)\n",
    "\n",
    "# Count significant singular values (threshold)\n",
    "threshold = 1e-10\n",
    "effective_rank = np.sum(s > threshold)\n",
    "\n",
    "print(f\"\\nEffective rank (œÉ > {threshold}): {effective_rank}\")\n",
    "print(f\"\\nInterpretation:\")\n",
    "print(f\"‚Üí Data lives in a {effective_rank}-dimensional subspace\")\n",
    "print(f\"‚Üí Intrinsic dimension ‚âà {effective_rank} (much less than {X.shape[1]})\")\n",
    "print(f\"‚Üí Can reduce from {X.shape[1]}D to {effective_rank}D with minimal loss\")\n",
    "\n",
    "# Variance explained\n",
    "variance_explained = np.cumsum(s**2) / np.sum(s**2)\n",
    "print(f\"\\nVariance explained by components:\")\n",
    "for i, var in enumerate(variance_explained):\n",
    "    print(f\"  First {i+1} components: {var*100:.2f}%\")\n",
    "\n",
    "print(f\"\\n‚Üí First 2 components capture {variance_explained[1]*100:.2f}% of variance\")\n",
    "print(f\"‚Üí Confirms true intrinsic dimension is 2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "norms-header",
   "metadata": {},
   "source": [
    "## 22. Norms\n",
    "\n",
    "A norm measures the \"size\" or \"length\" of a vector.\n",
    "\n",
    "**Properties:**\n",
    "1. Positivity: ||x|| ‚â• 0, equals 0 only if x = 0\n",
    "2. Scaling: ||Œ±x|| = |Œ±| ||x||\n",
    "3. Triangle inequality: ||x + y|| ‚â§ ||x|| + ||y||\n",
    "\n",
    "**ML Application:** Regularization, distance metrics, gradient clipping, loss functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "norms",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Vector Norms\n",
      "============================================================\n",
      "\n",
      "Vector x = [ 3 -4  0  2]\n",
      "\n",
      "------------------------------------------------------------\n",
      "L1 Norm (Manhattan Distance)\n",
      "------------------------------------------------------------\n",
      "\n",
      "Formula: ||x||‚ÇÅ = |x‚ÇÅ| + |x‚ÇÇ| + |x‚ÇÉ| + |x‚ÇÑ|\n",
      "       = |3| + |-4| + |0| + |2|\n",
      "       = 3 + 4 + 0 + 2\n",
      "       = 9\n",
      "\n",
      "NumPy: 9.0\n",
      "\n",
      "Interpretation:\n",
      "‚Üí Sum of absolute values\n",
      "‚Üí Distance in a grid (Manhattan blocks)\n",
      "‚Üí ML: Lasso regularization encourages sparsity\n",
      "\n",
      "------------------------------------------------------------\n",
      "L2 Norm (Euclidean Distance)\n",
      "------------------------------------------------------------\n",
      "\n",
      "Formula: ||x||‚ÇÇ = ‚àö(x‚ÇÅ¬≤ + x‚ÇÇ¬≤ + x‚ÇÉ¬≤ + x‚ÇÑ¬≤)\n",
      "       = ‚àö(3¬≤ + -4¬≤ + 0¬≤ + 2¬≤)\n",
      "       = ‚àö(9 + 16 + 0 + 4)\n",
      "       = ‚àö29\n",
      "       = 5.3852\n",
      "\n",
      "NumPy: 5.3852\n",
      "\n",
      "Interpretation:\n",
      "‚Üí Straight-line distance\n",
      "‚Üí Most common norm\n",
      "‚Üí ML: Ridge regularization, MSE loss\n",
      "\n",
      "------------------------------------------------------------\n",
      "L‚àû Norm (Maximum/Chebyshev)\n",
      "------------------------------------------------------------\n",
      "\n",
      "Formula: ||x||‚àû = max(|x‚ÇÅ|, |x‚ÇÇ|, |x‚ÇÉ|, |x‚ÇÑ|)\n",
      "       = max(|3|, |-4|, |0|, |2|)\n",
      "       = max(3, 4, 0, 2)\n",
      "       = 4\n",
      "\n",
      "NumPy: 4.0\n",
      "\n",
      "Interpretation:\n",
      "‚Üí Maximum absolute value\n",
      "‚Üí Largest single coordinate\n",
      "‚Üí ML: Adversarial robustness, gradient clipping\n",
      "\n",
      "============================================================\n",
      "Comparison of Norms\n",
      "============================================================\n",
      "\n",
      "For x = [ 3 -4  0  2]:\n",
      "  ||x||‚ÇÅ  = 9.0000\n",
      "  ||x||‚ÇÇ  = 5.3852\n",
      "  ||x||‚àû  = 4.0000\n",
      "\n",
      "General relationship: ||x||‚àû ‚â§ ||x||‚ÇÇ ‚â§ ||x||‚ÇÅ\n",
      "Verify: 4.0000 ‚â§ 5.3852 ‚â§ 9.0000 ‚úì\n",
      "\n",
      "============================================================\n",
      "Verifying Norm Properties\n",
      "============================================================\n",
      "\n",
      "Vectors: x = [ 3 -4  0  2], y = [ 1 -2  3 -1]\n",
      "Scalar: Œ± = 2.5\n",
      "\n",
      "1. Positivity: ||x|| ‚â• 0\n",
      "   ||x|| = 5.3852 ‚â• 0 ‚úì\n",
      "   ||0|| = 0.0000 = 0 ‚úì\n",
      "\n",
      "2. Scaling: ||Œ±x|| = |Œ±| ¬∑ ||x||\n",
      "   ||2.5x|| = 13.4629\n",
      "   |2.5| ¬∑ ||x|| = 13.4629\n",
      "   Equal? True ‚úì\n",
      "\n",
      "3. Triangle Inequality: ||x + y|| ‚â§ ||x|| + ||y||\n",
      "   ||x + y|| = 7.8740\n",
      "   ||x|| + ||y|| = 9.2581\n",
      "   7.8740 ‚â§ 9.2581? True ‚úì\n",
      "\n",
      "============================================================\n",
      "Matrix Norms\n",
      "============================================================\n",
      "\n",
      "Matrix A:\n",
      "[[1 2 3]\n",
      " [4 5 6]\n",
      " [7 8 9]]\n",
      "\n",
      "Frobenius Norm (like L2 for matrices):\n",
      "------------------------------------------------------------\n",
      "\n",
      "Formula: ||A||_F = ‚àö(Œ£·µ¢‚±º a·µ¢‚±º¬≤)\n",
      "       = ‚àö(285)\n",
      "       = 16.8819\n",
      "\n",
      "NumPy: 16.8819\n",
      "\n",
      "Interpretation:\n",
      "‚Üí Square root of sum of all squared elements\n",
      "‚Üí ML: Matrix regularization in neural networks\n",
      "\n",
      "Spectral Norm (largest singular value):\n",
      "------------------------------------------------------------\n",
      "\n",
      "Spectral norm: 16.8481\n",
      "Largest singular value: 16.8481\n",
      "Match? True ‚úì\n",
      "\n",
      "Interpretation:\n",
      "‚Üí Maximum 'stretching' of the matrix\n",
      "‚Üí ML: Lipschitz constraints, GAN training\n",
      "\n",
      "============================================================\n",
      "ML Application: L1 vs L2 Regularization\n",
      "============================================================\n",
      "\n",
      "Dense weights: [0.5 0.3 0.4 0.2 0.6]\n",
      "Sparse weights: [0.8 0.  0.  0.  0.9]\n",
      "\n",
      "L1 Penalty (Lasso):\n",
      "  Dense:  ||w||‚ÇÅ = 2.0000\n",
      "  Sparse: ||w||‚ÇÅ = 1.7000\n",
      "  ‚Üí L1 penalty similar for both\n",
      "\n",
      "L2 Penalty (Ridge):\n",
      "  Dense:  ||w||‚ÇÇ = 0.9487\n",
      "  Sparse: ||w||‚ÇÇ = 1.2042\n",
      "  ‚Üí L2 penalty LARGER for sparse (penalizes large values)\n",
      "\n",
      "Key Insight:\n",
      "‚Üí L1 promotes sparsity (many zeros)\n",
      "‚Üí L2 promotes small weights (spread out)\n",
      "‚Üí L1: Feature selection\n",
      "‚Üí L2: Weight decay\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Vector and Matrix Norms\n",
    "\n",
    "VECTOR NORMS:\n",
    "- L1 (Manhattan): ||x||‚ÇÅ = Œ£|x·µ¢|\n",
    "- L2 (Euclidean): ||x||‚ÇÇ = ‚àö(Œ£x·µ¢¬≤)\n",
    "- L‚àû (Max): ||x||‚àû = max|x·µ¢|\n",
    "- Lp (general): ||x||p = (Œ£|x·µ¢|·µñ)^(1/p)\n",
    "\n",
    "MATRIX NORMS:\n",
    "- Frobenius: ||A||_F = ‚àö(Œ£·µ¢‚±º a·µ¢‚±º¬≤)\n",
    "- Spectral: ||A||‚ÇÇ = largest singular value\n",
    "\n",
    "ML Applications:\n",
    "- L1: Lasso regression (sparsity)\n",
    "- L2: Ridge regression (weight decay)\n",
    "- Frobenius: Matrix regularization\n",
    "- Distance metrics in clustering\n",
    "\"\"\"\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"Vector Norms\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Example vector\n",
    "x = np.array([3, -4, 0, 2])\n",
    "\n",
    "print(f\"\\nVector x = {x}\")\n",
    "\n",
    "# L1 Norm (Manhattan/Taxicab)\n",
    "print(\"\\n\" + \"-\"*60)\n",
    "print(\"L1 Norm (Manhattan Distance)\")\n",
    "print(\"-\"*60)\n",
    "\n",
    "l1_manual = np.sum(np.abs(x))\n",
    "l1_numpy = np.linalg.norm(x, ord=1)\n",
    "\n",
    "print(f\"\\nFormula: ||x||‚ÇÅ = |x‚ÇÅ| + |x‚ÇÇ| + |x‚ÇÉ| + |x‚ÇÑ|\")\n",
    "print(f\"       = |{x[0]}| + |{x[1]}| + |{x[2]}| + |{x[3]}|\")\n",
    "print(f\"       = {np.abs(x[0])} + {np.abs(x[1])} + {np.abs(x[2])} + {np.abs(x[3])}\")\n",
    "print(f\"       = {l1_manual}\")\n",
    "\n",
    "print(f\"\\nNumPy: {l1_numpy}\")\n",
    "\n",
    "print(f\"\\nInterpretation:\")\n",
    "print(f\"‚Üí Sum of absolute values\")\n",
    "print(f\"‚Üí Distance in a grid (Manhattan blocks)\")\n",
    "print(f\"‚Üí ML: Lasso regularization encourages sparsity\")\n",
    "\n",
    "# L2 Norm (Euclidean)\n",
    "print(\"\\n\" + \"-\"*60)\n",
    "print(\"L2 Norm (Euclidean Distance)\")\n",
    "print(\"-\"*60)\n",
    "\n",
    "l2_manual = np.sqrt(np.sum(x**2))\n",
    "l2_numpy = np.linalg.norm(x, ord=2)\n",
    "l2_default = np.linalg.norm(x)  # Default is L2\n",
    "\n",
    "print(f\"\\nFormula: ||x||‚ÇÇ = ‚àö(x‚ÇÅ¬≤ + x‚ÇÇ¬≤ + x‚ÇÉ¬≤ + x‚ÇÑ¬≤)\")\n",
    "print(f\"       = ‚àö({x[0]}¬≤ + {x[1]}¬≤ + {x[2]}¬≤ + {x[3]}¬≤)\")\n",
    "print(f\"       = ‚àö({x[0]**2} + {x[1]**2} + {x[2]**2} + {x[3]**2})\")\n",
    "print(f\"       = ‚àö{np.sum(x**2)}\")\n",
    "print(f\"       = {l2_manual:.4f}\")\n",
    "\n",
    "print(f\"\\nNumPy: {l2_numpy:.4f}\")\n",
    "\n",
    "print(f\"\\nInterpretation:\")\n",
    "print(f\"‚Üí Straight-line distance\")\n",
    "print(f\"‚Üí Most common norm\")\n",
    "print(f\"‚Üí ML: Ridge regularization, MSE loss\")\n",
    "\n",
    "# L‚àû Norm (Max/Chebyshev)\n",
    "print(\"\\n\" + \"-\"*60)\n",
    "print(\"L‚àû Norm (Maximum/Chebyshev)\")\n",
    "print(\"-\"*60)\n",
    "\n",
    "linf_manual = np.max(np.abs(x))\n",
    "linf_numpy = np.linalg.norm(x, ord=np.inf)\n",
    "\n",
    "print(f\"\\nFormula: ||x||‚àû = max(|x‚ÇÅ|, |x‚ÇÇ|, |x‚ÇÉ|, |x‚ÇÑ|)\")\n",
    "print(f\"       = max(|{x[0]}|, |{x[1]}|, |{x[2]}|, |{x[3]}|)\")\n",
    "print(f\"       = max({np.abs(x[0])}, {np.abs(x[1])}, {np.abs(x[2])}, {np.abs(x[3])})\")\n",
    "print(f\"       = {linf_manual}\")\n",
    "\n",
    "print(f\"\\nNumPy: {linf_numpy}\")\n",
    "\n",
    "print(f\"\\nInterpretation:\")\n",
    "print(f\"‚Üí Maximum absolute value\")\n",
    "print(f\"‚Üí Largest single coordinate\")\n",
    "print(f\"‚Üí ML: Adversarial robustness, gradient clipping\")\n",
    "\n",
    "# Compare all norms\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Comparison of Norms\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"\\nFor x = {x}:\")\n",
    "print(f\"  ||x||‚ÇÅ  = {l1_numpy:.4f}\")\n",
    "print(f\"  ||x||‚ÇÇ  = {l2_numpy:.4f}\")\n",
    "print(f\"  ||x||‚àû  = {linf_numpy:.4f}\")\n",
    "\n",
    "print(f\"\\nGeneral relationship: ||x||‚àû ‚â§ ||x||‚ÇÇ ‚â§ ||x||‚ÇÅ\")\n",
    "print(f\"Verify: {linf_numpy:.4f} ‚â§ {l2_numpy:.4f} ‚â§ {l1_numpy:.4f} ‚úì\")\n",
    "\n",
    "# Norm properties\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Verifying Norm Properties\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Use L2 norm for demonstration\n",
    "y = np.array([1, -2, 3, -1])\n",
    "alpha = 2.5\n",
    "\n",
    "print(f\"\\nVectors: x = {x}, y = {y}\")\n",
    "print(f\"Scalar: Œ± = {alpha}\")\n",
    "\n",
    "# Property 1: Positivity\n",
    "print(\"\\n1. Positivity: ||x|| ‚â• 0\")\n",
    "norm_x = np.linalg.norm(x)\n",
    "print(f\"   ||x|| = {norm_x:.4f} ‚â• 0 ‚úì\")\n",
    "\n",
    "zero = np.zeros(4)\n",
    "norm_zero = np.linalg.norm(zero)\n",
    "print(f\"   ||0|| = {norm_zero:.4f} = 0 ‚úì\")\n",
    "\n",
    "# Property 2: Scaling\n",
    "print(\"\\n2. Scaling: ||Œ±x|| = |Œ±| ¬∑ ||x||\")\n",
    "norm_alpha_x = np.linalg.norm(alpha * x)\n",
    "scaled_norm = np.abs(alpha) * np.linalg.norm(x)\n",
    "print(f\"   ||{alpha}x|| = {norm_alpha_x:.4f}\")\n",
    "print(f\"   |{alpha}| ¬∑ ||x|| = {scaled_norm:.4f}\")\n",
    "print(f\"   Equal? {np.isclose(norm_alpha_x, scaled_norm)} ‚úì\")\n",
    "\n",
    "# Property 3: Triangle inequality\n",
    "print(\"\\n3. Triangle Inequality: ||x + y|| ‚â§ ||x|| + ||y||\")\n",
    "norm_sum = np.linalg.norm(x + y)\n",
    "sum_norms = np.linalg.norm(x) + np.linalg.norm(y)\n",
    "print(f\"   ||x + y|| = {norm_sum:.4f}\")\n",
    "print(f\"   ||x|| + ||y|| = {sum_norms:.4f}\")\n",
    "print(f\"   {norm_sum:.4f} ‚â§ {sum_norms:.4f}? {norm_sum <= sum_norms} ‚úì\")\n",
    "\n",
    "# Matrix Norms\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Matrix Norms\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "A = np.array([\n",
    "    [1, 2, 3],\n",
    "    [4, 5, 6],\n",
    "    [7, 8, 9]\n",
    "])\n",
    "\n",
    "print(f\"\\nMatrix A:\")\n",
    "print(A)\n",
    "\n",
    "# Frobenius norm\n",
    "print(\"\\nFrobenius Norm (like L2 for matrices):\")\n",
    "print(\"-\"*60)\n",
    "\n",
    "frob_manual = np.sqrt(np.sum(A**2))\n",
    "frob_numpy = np.linalg.norm(A, 'fro')\n",
    "\n",
    "print(f\"\\nFormula: ||A||_F = ‚àö(Œ£·µ¢‚±º a·µ¢‚±º¬≤)\")\n",
    "print(f\"       = ‚àö({np.sum(A**2)})\")\n",
    "print(f\"       = {frob_manual:.4f}\")\n",
    "\n",
    "print(f\"\\nNumPy: {frob_numpy:.4f}\")\n",
    "\n",
    "print(f\"\\nInterpretation:\")\n",
    "print(f\"‚Üí Square root of sum of all squared elements\")\n",
    "print(f\"‚Üí ML: Matrix regularization in neural networks\")\n",
    "\n",
    "# Spectral norm\n",
    "print(\"\\nSpectral Norm (largest singular value):\")\n",
    "print(\"-\"*60)\n",
    "\n",
    "spectral = np.linalg.norm(A, 2)\n",
    "U, s, Vt = np.linalg.svd(A)\n",
    "largest_sv = s[0]\n",
    "\n",
    "print(f\"\\nSpectral norm: {spectral:.4f}\")\n",
    "print(f\"Largest singular value: {largest_sv:.4f}\")\n",
    "print(f\"Match? {np.isclose(spectral, largest_sv)} ‚úì\")\n",
    "\n",
    "print(f\"\\nInterpretation:\")\n",
    "print(f\"‚Üí Maximum 'stretching' of the matrix\")\n",
    "print(f\"‚Üí ML: Lipschitz constraints, GAN training\")\n",
    "\n",
    "# ML Application: Regularization\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ML Application: L1 vs L2 Regularization\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Model weights\n",
    "weights_dense = np.array([0.5, 0.3, 0.4, 0.2, 0.6])\n",
    "weights_sparse = np.array([0.8, 0.0, 0.0, 0.0, 0.9])\n",
    "\n",
    "print(f\"\\nDense weights: {weights_dense}\")\n",
    "print(f\"Sparse weights: {weights_sparse}\")\n",
    "\n",
    "# L1 penalty (Lasso)\n",
    "l1_dense = np.linalg.norm(weights_dense, 1)\n",
    "l1_sparse = np.linalg.norm(weights_sparse, 1)\n",
    "\n",
    "print(f\"\\nL1 Penalty (Lasso):\")\n",
    "print(f\"  Dense:  ||w||‚ÇÅ = {l1_dense:.4f}\")\n",
    "print(f\"  Sparse: ||w||‚ÇÅ = {l1_sparse:.4f}\")\n",
    "print(f\"  ‚Üí L1 penalty similar for both\")\n",
    "\n",
    "# L2 penalty (Ridge)\n",
    "l2_dense = np.linalg.norm(weights_dense, 2)\n",
    "l2_sparse = np.linalg.norm(weights_sparse, 2)\n",
    "\n",
    "print(f\"\\nL2 Penalty (Ridge):\")\n",
    "print(f\"  Dense:  ||w||‚ÇÇ = {l2_dense:.4f}\")\n",
    "print(f\"  Sparse: ||w||‚ÇÇ = {l2_sparse:.4f}\")\n",
    "print(f\"  ‚Üí L2 penalty LARGER for sparse (penalizes large values)\")\n",
    "\n",
    "print(f\"\\nKey Insight:\")\n",
    "print(f\"‚Üí L1 promotes sparsity (many zeros)\")\n",
    "print(f\"‚Üí L2 promotes small weights (spread out)\")\n",
    "print(f\"‚Üí L1: Feature selection\")\n",
    "print(f\"‚Üí L2: Weight decay\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "inner-product-header",
   "metadata": {},
   "source": [
    "## 23. Inner, Outer, and Dot Products\n",
    "\n",
    "Different ways to combine vectors.\n",
    "\n",
    "**Dot Product:** Scalar result measuring alignment\n",
    "**Inner Product:** Generalized dot product (can have weights)\n",
    "**Outer Product:** Matrix result (all pairwise products)\n",
    "\n",
    "**ML Application:** Attention mechanisms, similarity metrics, neural network layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "products",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Inner, Outer, and Dot Products\n",
    "\n",
    "DOT PRODUCT (standard inner product):\n",
    "- u ¬∑ v = Œ£ u·µ¢v·µ¢\n",
    "- Result: scalar\n",
    "- Measures: alignment/similarity\n",
    "\n",
    "INNER PRODUCT (weighted):\n",
    "- ‚ü®u, v‚ü©_M = u·µÄMv\n",
    "- M: positive definite matrix\n",
    "- Generalizes dot product\n",
    "\n",
    "OUTER PRODUCT:\n",
    "- u ‚äó v = uv·µÄ\n",
    "- Result: matrix\n",
    "- Each element: u·µ¢v‚±º\n",
    "\n",
    "ML Applications:\n",
    "- Dot: cosine similarity, attention scores\n",
    "- Inner: Mahalanobis distance, kernel methods\n",
    "- Outer: rank-1 updates, covariance matrices\n",
    "\"\"\"\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"Dot Product\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "u = np.array([1, 2, 3])\n",
    "v = np.array([4, 5, 6])\n",
    "\n",
    "print(f\"\\nu = {u}\")\n",
    "print(f\"v = {v}\")\n",
    "\n",
    "# Compute dot product\n",
    "dot_manual = u[0]*v[0] + u[1]*v[1] + u[2]*v[2]\n",
    "dot_numpy = np.dot(u, v)\n",
    "dot_at = u @ v  # Alternative syntax\n",
    "\n",
    "print(f\"\\nFormula: u ¬∑ v = u‚ÇÅv‚ÇÅ + u‚ÇÇv‚ÇÇ + u‚ÇÉv‚ÇÉ\")\n",
    "print(f\"       = {u[0]}√ó{v[0]} + {u[1]}√ó{v[1]} + {u[2]}√ó{v[2]}\")\n",
    "print(f\"       = {u[0]*v[0]} + {u[1]*v[1]} + {u[2]*v[2]}\")\n",
    "print(f\"       = {dot_manual}\")\n",
    "\n",
    "print(f\"\\nNumPy (np.dot): {dot_numpy}\")\n",
    "print(f\"NumPy (@ operator): {dot_at}\")\n",
    "\n",
    "# Geometric interpretation\n",
    "print(\"\\n\" + \"-\"*60)\n",
    "print(\"Geometric Interpretation\")\n",
    "print(\"-\"*60)\n",
    "\n",
    "# Dot product = ||u|| ||v|| cos(Œ∏)\n",
    "norm_u = np.linalg.norm(u)\n",
    "norm_v = np.linalg.norm(v)\n",
    "cos_theta = dot_numpy / (norm_u * norm_v)\n",
    "theta_rad = np.arccos(cos_theta)\n",
    "theta_deg = np.degrees(theta_rad)\n",
    "\n",
    "print(f\"\\nFormula: u ¬∑ v = ||u|| ||v|| cos(Œ∏)\")\n",
    "print(f\"\\n||u|| = {norm_u:.4f}\")\n",
    "print(f\"||v|| = {norm_v:.4f}\")\n",
    "print(f\"\\ncos(Œ∏) = (u ¬∑ v) / (||u|| ||v||)\")\n",
    "print(f\"       = {dot_numpy} / ({norm_u:.4f} √ó {norm_v:.4f})\")\n",
    "print(f\"       = {cos_theta:.4f}\")\n",
    "\n",
    "print(f\"\\nAngle Œ∏ = {theta_deg:.2f}¬∞\")\n",
    "\n",
    "print(f\"\\nInterpretation:\")\n",
    "print(f\"‚Üí Measures how much vectors point in same direction\")\n",
    "print(f\"‚Üí Positive: vectors point similar direction\")\n",
    "print(f\"‚Üí Zero: vectors are perpendicular\")\n",
    "print(f\"‚Üí Negative: vectors point opposite directions\")\n",
    "\n",
    "# Special cases\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Special Cases\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Orthogonal vectors\n",
    "print(\"\\nCase 1: Orthogonal Vectors (perpendicular)\")\n",
    "a = np.array([1, 0])\n",
    "b = np.array([0, 1])\n",
    "dot_ab = np.dot(a, b)\n",
    "\n",
    "print(f\"a = {a}\")\n",
    "print(f\"b = {b}\")\n",
    "print(f\"a ¬∑ b = {dot_ab}\")\n",
    "print(f\"‚Üí Dot product = 0 ‚Üí vectors are ORTHOGONAL ‚úì\")\n",
    "\n",
    "# Parallel vectors\n",
    "print(\"\\nCase 2: Parallel Vectors (same direction)\")\n",
    "a = np.array([1, 2, 3])\n",
    "b = 2 * a  # Parallel\n",
    "dot_ab = np.dot(a, b)\n",
    "norm_prod = np.linalg.norm(a) * np.linalg.norm(b)\n",
    "\n",
    "print(f\"a = {a}\")\n",
    "print(f\"b = {b} = 2a\")\n",
    "print(f\"a ¬∑ b = {dot_ab:.4f}\")\n",
    "print(f\"||a|| ||b|| = {norm_prod:.4f}\")\n",
    "print(f\"‚Üí a ¬∑ b = ||a|| ||b|| ‚Üí cos(Œ∏) = 1 ‚Üí Œ∏ = 0¬∞ ‚úì\")\n",
    "\n",
    "# Anti-parallel vectors\n",
    "print(\"\\nCase 3: Anti-parallel Vectors (opposite direction)\")\n",
    "a = np.array([1, 2, 3])\n",
    "b = -1 * a  # Opposite\n",
    "dot_ab = np.dot(a, b)\n",
    "norm_prod = np.linalg.norm(a) * np.linalg.norm(b)\n",
    "\n",
    "print(f\"a = {a}\")\n",
    "print(f\"b = {b} = -a\")\n",
    "print(f\"a ¬∑ b = {dot_ab:.4f}\")\n",
    "print(f\"-||a|| ||b|| = {-norm_prod:.4f}\")\n",
    "print(f\"‚Üí a ¬∑ b = -||a|| ||b|| ‚Üí cos(Œ∏) = -1 ‚Üí Œ∏ = 180¬∞ ‚úì\")\n",
    "\n",
    "# Outer Product\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Outer Product\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "u = np.array([1, 2, 3])\n",
    "v = np.array([4, 5])\n",
    "\n",
    "print(f\"\\nu = {u} (shape: {u.shape})\")\n",
    "print(f\"v = {v} (shape: {v.shape})\")\n",
    "\n",
    "# Compute outer product\n",
    "outer = np.outer(u, v)\n",
    "\n",
    "print(f\"\\nOuter product u ‚äó v:\")\n",
    "print(outer)\n",
    "print(f\"Shape: {outer.shape}\")\n",
    "\n",
    "print(f\"\\nElement-wise breakdown:\")\n",
    "print(f\"u ‚äó v = [[u‚ÇÅv‚ÇÅ, u‚ÇÅv‚ÇÇ],\")\n",
    "print(f\"         [u‚ÇÇv‚ÇÅ, u‚ÇÇv‚ÇÇ],\")\n",
    "print(f\"         [u‚ÇÉv‚ÇÅ, u‚ÇÉv‚ÇÇ]]\")\n",
    "print(f\"\")\n",
    "print(f\"      = [[{u[0]}√ó{v[0]}, {u[0]}√ó{v[1]}],\")\n",
    "print(f\"         [{u[1]}√ó{v[0]}, {u[1]}√ó{v[1]}],\")\n",
    "print(f\"         [{u[2]}√ó{v[0]}, {u[2]}√ó{v[1]}]]\")\n",
    "print(f\"\")\n",
    "print(f\"      = {outer}\")\n",
    "\n",
    "print(f\"\\nInterpretation:\")\n",
    "print(f\"‚Üí Creates a matrix from two vectors\")\n",
    "print(f\"‚Üí Each element is u·µ¢v‚±º\")\n",
    "print(f\"‚Üí Rank-1 matrix (all rows are multiples of each other)\")\n",
    "\n",
    "# Verify rank-1\n",
    "rank = np.linalg.matrix_rank(outer)\n",
    "print(f\"\\nRank of outer product: {rank}\")\n",
    "print(f\"‚Üí Always rank-1 (except for zero vectors) ‚úì\")\n",
    "\n",
    "# Inner Product (weighted)\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Inner Product (Weighted)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "u = np.array([1, 2, 3])\n",
    "v = np.array([4, 5, 6])\n",
    "\n",
    "# Weight matrix (positive definite)\n",
    "M = np.array([\n",
    "    [2, 0, 0],\n",
    "    [0, 3, 0],\n",
    "    [0, 0, 1]\n",
    "])\n",
    "\n",
    "print(f\"\\nu = {u}\")\n",
    "print(f\"v = {v}\")\n",
    "print(f\"\\nWeight matrix M:\")\n",
    "print(M)\n",
    "\n",
    "# Standard dot product\n",
    "standard_dot = np.dot(u, v)\n",
    "\n",
    "# Weighted inner product: ‚ü®u, v‚ü©_M = u·µÄMv\n",
    "weighted_inner = u @ M @ v\n",
    "\n",
    "print(f\"\\nStandard dot product: u ¬∑ v = {standard_dot}\")\n",
    "print(f\"Weighted inner product: ‚ü®u, v‚ü©_M = {weighted_inner}\")\n",
    "\n",
    "print(f\"\\nComputation:\")\n",
    "print(f\"‚ü®u, v‚ü©_M = u·µÄMv\")\n",
    "Mv = M @ v\n",
    "print(f\"         = u ¬∑ (Mv)\")\n",
    "print(f\"         = {u} ¬∑ {Mv}\")\n",
    "print(f\"         = {weighted_inner}\")\n",
    "\n",
    "print(f\"\\nInterpretation:\")\n",
    "print(f\"‚Üí M gives different weights to different dimensions\")\n",
    "print(f\"‚Üí When M = I, reduces to standard dot product\")\n",
    "print(f\"‚Üí ML: Mahalanobis distance uses covariance as M\")\n",
    "\n",
    "# ML Application: Cosine Similarity\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ML Application: Cosine Similarity\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Document vectors (word counts)\n",
    "doc1 = np.array([2, 3, 1, 0])  # Document 1\n",
    "doc2 = np.array([1, 2, 2, 1])  # Document 2 (similar)\n",
    "doc3 = np.array([0, 0, 3, 4])  # Document 3 (different)\n",
    "\n",
    "def cosine_similarity(a, b):\n",
    "    \"\"\"Compute cosine similarity: cos(Œ∏) = (a¬∑b) / (||a|| ||b||)\"\"\"\n",
    "    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))\n",
    "\n",
    "sim_12 = cosine_similarity(doc1, doc2)\n",
    "sim_13 = cosine_similarity(doc1, doc3)\n",
    "sim_23 = cosine_similarity(doc2, doc3)\n",
    "\n",
    "print(f\"\\nDocument vectors (word counts):\")\n",
    "print(f\"Doc 1: {doc1}\")\n",
    "print(f\"Doc 2: {doc2}\")\n",
    "print(f\"Doc 3: {doc3}\")\n",
    "\n",
    "print(f\"\\nCosine Similarities:\")\n",
    "print(f\"Doc 1 vs Doc 2: {sim_12:.4f}\")\n",
    "print(f\"Doc 1 vs Doc 3: {sim_13:.4f}\")\n",
    "print(f\"Doc 2 vs Doc 3: {sim_23:.4f}\")\n",
    "\n",
    "print(f\"\\nInterpretation:\")\n",
    "print(f\"‚Üí Values range from -1 to 1\")\n",
    "print(f\"‚Üí 1: identical direction (very similar)\")\n",
    "print(f\"‚Üí 0: orthogonal (unrelated)\")\n",
    "print(f\"‚Üí -1: opposite direction (very different)\")\n",
    "\n",
    "print(f\"\\n‚Üí Doc 1 and Doc 2 are more similar ({sim_12:.4f})\")\n",
    "print(f\"‚Üí Doc 1 and Doc 3 are less similar ({sim_13:.4f})\")\n",
    "\n",
    "# ML Application: Attention Mechanism\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ML Application: Attention Scores (Simplified)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Query and Key vectors\n",
    "query = np.array([1.0, 0.5, 0.2])\n",
    "key1 = np.array([0.9, 0.6, 0.1])  # Similar to query\n",
    "key2 = np.array([0.1, 0.2, 0.9])  # Different from query\n",
    "\n",
    "print(f\"\\nQuery: {query}\")\n",
    "print(f\"Key 1: {key1} (similar to query)\")\n",
    "print(f\"Key 2: {key2} (different from query)\")\n",
    "\n",
    "# Attention scores (dot product)\n",
    "score1 = np.dot(query, key1)\n",
    "score2 = np.dot(query, key2)\n",
    "\n",
    "print(f\"\\nAttention scores (before softmax):\")\n",
    "print(f\"Score 1 (Query ¬∑ Key1): {score1:.4f}\")\n",
    "print(f\"Score 2 (Query ¬∑ Key2): {score2:.4f}\")\n",
    "\n",
    "# Softmax normalization\n",
    "scores = np.array([score1, score2])\n",
    "attention_weights = np.exp(scores) / np.sum(np.exp(scores))\n",
    "\n",
    "print(f\"\\nAttention weights (after softmax):\")\n",
    "print(f\"Weight 1: {attention_weights[0]:.4f}\")\n",
    "print(f\"Weight 2: {attention_weights[1]:.4f}\")\n",
    "print(f\"Sum: {np.sum(attention_weights):.4f}\")\n",
    "\n",
    "print(f\"\\nInterpretation:\")\n",
    "print(f\"‚Üí Higher dot product = higher attention\")\n",
    "print(f\"‚Üí Query attends more to Key1 ({attention_weights[0]*100:.1f}%)\")\n",
    "print(f\"‚Üí Used in Transformers, BERT, GPT, etc.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83522dc7",
   "metadata": {},
   "source": [
    "# üìä Summary Table: Linear Algebra Concepts (Part 3)\n",
    "\n",
    "| Concept | Definition | Key Property | ML Application |\n",
    "|---------|------------|--------------|----------------|\n",
    "| **Dimension** | Number of vectors in any basis of a space | All bases have same size; dimension = rank | Feature dimensionality, model capacity, intrinsic data dimension |\n",
    "| **Norms** | Measures \"size\" or \"length\" of vectors/matrices | L‚ÇÅ (Manhattan), L‚ÇÇ (Euclidean), L‚àû (Max), Frobenius (matrix) | Regularization (Lasso/Ridge), distance metrics, gradient clipping |\n",
    "| **Dot Product** | Scalar: u¬∑v = Œ£u·µ¢v·µ¢ = \\|\\|u\\|\\|\\|v\\|cos(Œ∏) | Measures alignment (cosine similarity) | Attention mechanisms, similarity scores, neural activations |\n",
    "| **Outer Product** | Matrix: u‚äóv = uv·µÄ (each element u·µ¢v‚±º) | Always rank-1 (except zero vectors) | Rank-1 updates, covariance estimation |\n",
    "| **Inner Product** | Generalized: ‚ü®u,v‚ü©_M = u·µÄMv (M positive definite) | Reduces to dot product when M=I | Mahalanobis distance, kernel methods |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "continue-part3",
   "metadata": {},
   "source": [
    "## Part 3 Progress\n",
    "\n",
    "**Completed:**\n",
    "- Dimension of Vector Space (21)\n",
    "- Norms (22) - L1, L2, L‚àû, Frobenius, Spectral\n",
    "- Inner, Outer, Dot Products (23)\n",
    "\n",
    "**Next Part 4 will cover:**\n",
    "- Projection Matrices\n",
    "- Orthogonality & Orthonormal Basis\n",
    "- Eigenvalues & Eigenvectors\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
