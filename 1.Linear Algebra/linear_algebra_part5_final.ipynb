{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# Linear Algebra for AI/ML - Part 5: SVD, Decompositions & Advanced Topics\n",
    "\n",
    "This final notebook covers SVD, matrix decompositions, and advanced concepts.\n",
    "\n",
    "**Prerequisites:** Complete Parts 1-4 first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "setup",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NumPy version: 2.2.6\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Setup: Import Required Libraries\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import linalg\n",
    "from PIL import Image\n",
    "\n",
    "np.set_printoptions(precision=4, suppress=True)\n",
    "print(f\"NumPy version: {np.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "svd-header",
   "metadata": {},
   "source": [
    "## 27. Singular Value Decomposition (SVD)\n",
    "\n",
    "**Every matrix** A (m×n) can be factored as:\n",
    "\n",
    "**A = UΣVᵀ**\n",
    "\n",
    "Where:\n",
    "- **U** (m×m): Left singular vectors (orthogonal)\n",
    "- **Σ** (m×n): Diagonal matrix of singular values (σ₁ ≥ σ₂ ≥ ... ≥ 0)\n",
    "- **V** (n×n): Right singular vectors (orthogonal)\n",
    "\n",
    "**ML Applications:** PCA, image compression, recommender systems, NLP (LSA), matrix approximation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "svd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Singular Value Decomposition (SVD)\n",
      "============================================================\n",
      "\n",
      "Matrix A (2×3):\n",
      "[[ 3  2  2]\n",
      " [ 2  3 -2]]\n",
      "Shape: (2, 3)\n",
      "\n",
      "------------------------------------------------------------\n",
      "SVD Components: A = UΣVᵀ\n",
      "------------------------------------------------------------\n",
      "\n",
      "U (left singular vectors, (2, 2)):\n",
      "[[-0.7071 -0.7071]\n",
      " [-0.7071  0.7071]]\n",
      "\n",
      "Singular values σ (2):\n",
      "[5. 3.]\n",
      "\n",
      "Vᵀ (right singular vectors transposed, (3, 3)):\n",
      "[[-0.7071 -0.7071 -0.    ]\n",
      " [-0.2357  0.2357 -0.9428]\n",
      " [-0.6667  0.6667  0.3333]]\n",
      "\n",
      "Σ matrix ((2, 3)):\n",
      "[[5. 0. 0.]\n",
      " [0. 3. 0.]]\n",
      "\n",
      "Reconstructed A = UΣVᵀ:\n",
      "[[ 3.  2.  2.]\n",
      " [ 2.  3. -2.]]\n",
      "\n",
      "Original A:\n",
      "[[ 3  2  2]\n",
      " [ 2  3 -2]]\n",
      "\n",
      "Match? True ✓\n",
      "\n",
      "------------------------------------------------------------\n",
      "Verifying Orthogonality\n",
      "------------------------------------------------------------\n",
      "\n",
      "UᵀU (should be I):\n",
      "[[1. 0.]\n",
      " [0. 1.]]\n",
      "Is identity? True ✓\n",
      "\n",
      "VᵀV (should be I):\n",
      "[[ 1.  0.  0.]\n",
      " [ 0.  1. -0.]\n",
      " [ 0. -0.  1.]]\n",
      "Is identity? True ✓\n",
      "\n",
      "------------------------------------------------------------\n",
      "Matrix Rank from SVD\n",
      "------------------------------------------------------------\n",
      "\n",
      "Singular values: [5. 3.]\n",
      "Non-zero singular values: 2\n",
      "Rank from numpy: 2\n",
      "Match? True ✓\n",
      "\n",
      "============================================================\n",
      "Low-Rank Approximation\n",
      "============================================================\n",
      "\n",
      "Approximating with top 1 singular value(s)\n",
      "\n",
      "Rank-1 approximation:\n",
      "[[2.5 2.5 0. ]\n",
      " [2.5 2.5 0. ]]\n",
      "\n",
      "Original A:\n",
      "[[ 3  2  2]\n",
      " [ 2  3 -2]]\n",
      "\n",
      "Approximation Error:\n",
      "Frobenius norm: 3.0000\n",
      "Relative error: 51.45%\n",
      "\n",
      "============================================================\n",
      "SVD vs Eigendecomposition\n",
      "============================================================\n",
      "\n",
      "Symmetric matrix S:\n",
      "[[4 2]\n",
      " [2 3]]\n",
      "\n",
      "Singular values: [5.5616 1.4384]\n",
      "Eigenvalues: [5.5616 1.4384]\n",
      "\n",
      "For symmetric matrices:\n",
      "→ Singular values = |eigenvalues|\n",
      "→ U = V = eigenvectors (up to sign)\n",
      "Match? True ✓\n",
      "\n",
      "============================================================\n",
      "ML Application: Image Compression with SVD\n",
      "============================================================\n",
      "\n",
      "Original 'image' (8×8):\n",
      "[[102 179  92  14 106  71 188  20]\n",
      " [102 121 210 214  74 202  87 116]\n",
      " [ 99 103 151 130 149  52   1  87]\n",
      " [235 157  37 129 191 187  20 160]\n",
      " [203  57  21 252 235  88  48 218]\n",
      " [ 58 254 169 255 219 187 207  14]\n",
      " [189 189 174 189  50 107  54 243]\n",
      " [ 63 248 130 228  50 134  20  72]]\n",
      "\n",
      "Singular values:\n",
      "[1094.0275  343.5379  244.8231  190.302   140.0478  117.7146  104.9556\n",
      "   11.3808]\n",
      "\n",
      "Compression with different ranks:\n",
      "\n",
      "Rank-1:\n",
      "  Error (Frobenius): 508.71\n",
      "  Variance explained: 82.22%\n",
      "  Compression ratio: 3.76x\n",
      "\n",
      "Rank-2:\n",
      "  Error (Frobenius): 375.19\n",
      "  Variance explained: 90.33%\n",
      "  Compression ratio: 1.88x\n",
      "\n",
      "Rank-4:\n",
      "  Error (Frobenius): 211.22\n",
      "  Variance explained: 96.94%\n",
      "  Compression ratio: 0.94x\n",
      "\n",
      "Rank-8:\n",
      "  Error (Frobenius): 0.00\n",
      "  Variance explained: 100.00%\n",
      "  Compression ratio: 0.47x\n",
      "\n",
      "→ SVD finds best low-rank approximation\n",
      "→ Trade-off: quality vs. compression\n",
      "→ Used in JPEG, video compression, etc.\n",
      "\n",
      "============================================================\n",
      "ML Application: PCA via SVD\n",
      "============================================================\n",
      "\n",
      "Data shape: (100, 3)\n",
      "\n",
      "Principal Components (columns of V):\n",
      "[[-0.6075  0.7583 -0.2365]\n",
      " [-0.685  -0.6509 -0.3273]\n",
      " [-0.4021 -0.0368  0.9149]]\n",
      "\n",
      "Variance explained by each component:\n",
      "  PC1: 1.6968 (94.32%)\n",
      "  PC2: 0.0932 (5.18%)\n",
      "  PC3: 0.0090 (0.50%)\n",
      "\n",
      "Transformed data shape: (100, 3)\n",
      "\n",
      "Verify: X @ V = U @ Σ\n",
      "Match? True ✓\n",
      "\n",
      "→ SVD is the standard way to compute PCA\n",
      "→ More numerically stable than eigendecomposition\n",
      "→ Works even when XᵀX is singular\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Singular Value Decomposition (SVD)\n",
    "\n",
    "Formula: A = UΣVᵀ\n",
    "\n",
    "Components:\n",
    "- U: m×m orthogonal matrix (left singular vectors)\n",
    "  - Columns are eigenvectors of AAᵀ\n",
    "  - Output directions/principal directions\n",
    "\n",
    "- Σ: m×n diagonal matrix (singular values)\n",
    "  - σᵢ ≥ 0 (always non-negative)\n",
    "  - Ordered: σ₁ ≥ σ₂ ≥ ... ≥ 0\n",
    "  - Strength of each direction\n",
    "\n",
    "- V: n×n orthogonal matrix (right singular vectors)\n",
    "  - Columns are eigenvectors of AᵀA\n",
    "  - Input directions/feature directions\n",
    "\n",
    "Key Facts:\n",
    "1. ALWAYS exists (unlike eigendecomposition)\n",
    "2. Works for ANY matrix (rectangular, singular, etc.)\n",
    "3. Generalizes eigendecomposition\n",
    "4. rank(A) = number of non-zero singular values\n",
    "5. Best low-rank approximation\n",
    "\n",
    "ML Applications:\n",
    "- PCA (equivalent to SVD on centered data)\n",
    "- Image compression\n",
    "- Recommender systems (matrix factorization)\n",
    "- Latent Semantic Analysis (NLP)\n",
    "- Pseudo-inverse computation\n",
    "\"\"\"\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"Singular Value Decomposition (SVD)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Example matrix\n",
    "A = np.array([\n",
    "    [3, 2, 2],\n",
    "    [2, 3, -2]\n",
    "])\n",
    "\n",
    "print(f\"\\nMatrix A (2×3):\")\n",
    "print(A)\n",
    "print(f\"Shape: {A.shape}\")\n",
    "\n",
    "# Compute SVD\n",
    "U, s, Vt = np.linalg.svd(A, full_matrices=True)\n",
    "\n",
    "print(f\"\\n\" + \"-\"*60)\n",
    "print(\"SVD Components: A = UΣVᵀ\")\n",
    "print(\"-\"*60)\n",
    "\n",
    "print(f\"\\nU (left singular vectors, {U.shape}):\")\n",
    "print(U)\n",
    "\n",
    "print(f\"\\nSingular values σ ({len(s)}):\")\n",
    "print(s)\n",
    "\n",
    "print(f\"\\nVᵀ (right singular vectors transposed, {Vt.shape}):\")\n",
    "print(Vt)\n",
    "\n",
    "# Reconstruct Σ matrix\n",
    "Sigma = np.zeros(A.shape)\n",
    "np.fill_diagonal(Sigma, s)\n",
    "\n",
    "print(f\"\\nΣ matrix ({Sigma.shape}):\")\n",
    "print(Sigma)\n",
    "\n",
    "# Verify reconstruction\n",
    "A_reconstructed = U @ Sigma @ Vt\n",
    "\n",
    "print(f\"\\nReconstructed A = UΣVᵀ:\")\n",
    "print(A_reconstructed)\n",
    "\n",
    "print(f\"\\nOriginal A:\")\n",
    "print(A)\n",
    "\n",
    "print(f\"\\nMatch? {np.allclose(A_reconstructed, A)} ✓\")\n",
    "\n",
    "# Verify orthogonality\n",
    "print(\"\\n\" + \"-\"*60)\n",
    "print(\"Verifying Orthogonality\")\n",
    "print(\"-\"*60)\n",
    "\n",
    "UTU = U.T @ U\n",
    "VTV = Vt.T @ Vt\n",
    "\n",
    "print(f\"\\nUᵀU (should be I):\")\n",
    "print(UTU)\n",
    "print(f\"Is identity? {np.allclose(UTU, np.eye(U.shape[0]))} ✓\")\n",
    "\n",
    "print(f\"\\nVᵀV (should be I):\")\n",
    "print(VTV)\n",
    "print(f\"Is identity? {np.allclose(VTV, np.eye(Vt.shape[0]))} ✓\")\n",
    "\n",
    "# Rank from singular values\n",
    "print(\"\\n\" + \"-\"*60)\n",
    "print(\"Matrix Rank from SVD\")\n",
    "print(\"-\"*60)\n",
    "\n",
    "rank_svd = np.sum(s > 1e-10)  # Count non-zero singular values\n",
    "rank_numpy = np.linalg.matrix_rank(A)\n",
    "\n",
    "print(f\"\\nSingular values: {s}\")\n",
    "print(f\"Non-zero singular values: {rank_svd}\")\n",
    "print(f\"Rank from numpy: {rank_numpy}\")\n",
    "print(f\"Match? {rank_svd == rank_numpy} ✓\")\n",
    "\n",
    "# Low-Rank Approximation\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Low-Rank Approximation\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Keep only top k singular values\n",
    "k = 1  # Rank-1 approximation\n",
    "\n",
    "print(f\"\\nApproximating with top {k} singular value(s)\")\n",
    "\n",
    "# Zero out smaller singular values\n",
    "Sigma_k = Sigma.copy()\n",
    "Sigma_k[:, k:] = 0\n",
    "\n",
    "A_k = U @ Sigma_k @ Vt\n",
    "\n",
    "print(f\"\\nRank-{k} approximation:\")\n",
    "print(A_k)\n",
    "\n",
    "print(f\"\\nOriginal A:\")\n",
    "print(A)\n",
    "\n",
    "# Approximation error (Frobenius norm)\n",
    "error = np.linalg.norm(A - A_k, 'fro')\n",
    "original_norm = np.linalg.norm(A, 'fro')\n",
    "relative_error = error / original_norm\n",
    "\n",
    "print(f\"\\nApproximation Error:\")\n",
    "print(f\"Frobenius norm: {error:.4f}\")\n",
    "print(f\"Relative error: {relative_error*100:.2f}%\")\n",
    "\n",
    "# Connection to Eigendecomposition\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SVD vs Eigendecomposition\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# For symmetric matrix, SVD = eigendecomposition\n",
    "S = np.array([\n",
    "    [4, 2],\n",
    "    [2, 3]\n",
    "])\n",
    "\n",
    "print(f\"\\nSymmetric matrix S:\")\n",
    "print(S)\n",
    "\n",
    "# SVD\n",
    "U_s, s_s, Vt_s = np.linalg.svd(S)\n",
    "\n",
    "# Eigendecomposition\n",
    "eigenvalues, eigenvectors = np.linalg.eigh(S)\n",
    "# Sort descending\n",
    "idx = eigenvalues.argsort()[::-1]\n",
    "eigenvalues = eigenvalues[idx]\n",
    "eigenvectors = eigenvectors[:, idx]\n",
    "\n",
    "print(f\"\\nSingular values: {s_s}\")\n",
    "print(f\"Eigenvalues: {eigenvalues}\")\n",
    "print(f\"\\nFor symmetric matrices:\")\n",
    "print(f\"→ Singular values = |eigenvalues|\")\n",
    "print(f\"→ U = V = eigenvectors (up to sign)\")\n",
    "print(f\"Match? {np.allclose(s_s, np.abs(eigenvalues))} ✓\")\n",
    "\n",
    "# ML Application: Image Compression\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ML Application: Image Compression with SVD\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Create simple synthetic \"image\" (8x8 grayscale)\n",
    "np.random.seed(42)\n",
    "image = np.random.randint(0, 256, (8, 8)).astype(float)\n",
    "\n",
    "print(f\"\\nOriginal 'image' (8×8):\")\n",
    "print(image.astype(int))\n",
    "\n",
    "# SVD\n",
    "U_img, s_img, Vt_img = np.linalg.svd(image, full_matrices=False)\n",
    "\n",
    "print(f\"\\nSingular values:\")\n",
    "print(s_img)\n",
    "\n",
    "# Compress with different ranks\n",
    "ranks_to_try = [1, 2, 4, 8]\n",
    "\n",
    "print(f\"\\nCompression with different ranks:\")\n",
    "for k in ranks_to_try:\n",
    "    if k > len(s_img):\n",
    "        break\n",
    "    \n",
    "    # Reconstruct with top k components\n",
    "    S_k = np.diag(s_img[:k])\n",
    "    image_k = U_img[:, :k] @ S_k @ Vt_img[:k, :]\n",
    "    \n",
    "    # Error\n",
    "    error = np.linalg.norm(image - image_k, 'fro')\n",
    "    \n",
    "    # Compression ratio\n",
    "    original_size = image.shape[0] * image.shape[1]\n",
    "    compressed_size = k * (image.shape[0] + image.shape[1] + 1)\n",
    "    compression_ratio = original_size / compressed_size\n",
    "    \n",
    "    # Variance explained\n",
    "    variance_explained = np.sum(s_img[:k]**2) / np.sum(s_img**2)\n",
    "    \n",
    "    print(f\"\\nRank-{k}:\")\n",
    "    print(f\"  Error (Frobenius): {error:.2f}\")\n",
    "    print(f\"  Variance explained: {variance_explained*100:.2f}%\")\n",
    "    print(f\"  Compression ratio: {compression_ratio:.2f}x\")\n",
    "\n",
    "print(f\"\\n→ SVD finds best low-rank approximation\")\n",
    "print(f\"→ Trade-off: quality vs. compression\")\n",
    "print(f\"→ Used in JPEG, video compression, etc.\")\n",
    "\n",
    "# ML Application: PCA via SVD\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ML Application: PCA via SVD\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Generate data\n",
    "np.random.seed(42)\n",
    "n_samples = 100\n",
    "X = np.random.randn(n_samples, 3)  # 3 features\n",
    "\n",
    "# Add correlation\n",
    "X[:, 1] = X[:, 0] + 0.5 * np.random.randn(n_samples)\n",
    "X[:, 2] = 0.3 * X[:, 0] + 0.3 * X[:, 1] + 0.1 * np.random.randn(n_samples)\n",
    "\n",
    "print(f\"\\nData shape: {X.shape}\")\n",
    "\n",
    "# Center the data\n",
    "X_centered = X - X.mean(axis=0)\n",
    "\n",
    "# SVD (PCA via SVD)\n",
    "U_pca, s_pca, Vt_pca = np.linalg.svd(X_centered, full_matrices=False)\n",
    "\n",
    "# Principal components = Vᵀ (right singular vectors)\n",
    "principal_components = Vt_pca.T\n",
    "\n",
    "print(f\"\\nPrincipal Components (columns of V):\")\n",
    "print(principal_components)\n",
    "\n",
    "# Variance explained\n",
    "# Variance = (singular values)² / (n-1)\n",
    "explained_variance = (s_pca ** 2) / (n_samples - 1)\n",
    "total_variance = explained_variance.sum()\n",
    "explained_variance_ratio = explained_variance / total_variance\n",
    "\n",
    "print(f\"\\nVariance explained by each component:\")\n",
    "for i, (var, ratio) in enumerate(zip(explained_variance, explained_variance_ratio)):\n",
    "    print(f\"  PC{i+1}: {var:.4f} ({ratio*100:.2f}%)\")\n",
    "\n",
    "# Transform data\n",
    "X_pca = X_centered @ principal_components\n",
    "\n",
    "print(f\"\\nTransformed data shape: {X_pca.shape}\")\n",
    "\n",
    "# Alternatively: X_pca = U @ Σ\n",
    "Sigma_pca = np.diag(s_pca)\n",
    "X_pca_alt = U_pca @ Sigma_pca\n",
    "\n",
    "print(f\"\\nVerify: X @ V = U @ Σ\")\n",
    "print(f\"Match? {np.allclose(X_pca, X_pca_alt)} ✓\")\n",
    "\n",
    "print(f\"\\n→ SVD is the standard way to compute PCA\")\n",
    "print(f\"→ More numerically stable than eigendecomposition\")\n",
    "print(f\"→ Works even when XᵀX is singular\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cholesky-header",
   "metadata": {},
   "source": [
    "## 28. Cholesky Decomposition\n",
    "\n",
    "For symmetric positive definite matrix A:\n",
    "\n",
    "**A = LLᵀ**\n",
    "\n",
    "Where L is lower triangular with positive diagonal.\n",
    "\n",
    "**ML Application:** Sampling from multivariate Gaussian, solving linear systems efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cholesky",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Cholesky Decomposition\n",
      "============================================================\n",
      "\n",
      "Symmetric positive definite matrix A:\n",
      "[[ 4  2]\n",
      " [ 2 10]]\n",
      "\n",
      "Verify symmetric: A = Aᵀ?\n",
      "True ✓\n",
      "\n",
      "Eigenvalues: [ 3.3944 10.6056]\n",
      "All positive? True ✓\n",
      "→ Matrix is positive definite ✓\n",
      "\n",
      "Cholesky factor L (lower triangular):\n",
      "[[2. 0.]\n",
      " [1. 3.]]\n",
      "\n",
      "Reconstructed A = LLᵀ:\n",
      "[[ 4.  2.]\n",
      " [ 2. 10.]]\n",
      "\n",
      "Original A:\n",
      "[[ 4  2]\n",
      " [ 2 10]]\n",
      "\n",
      "Match? True ✓\n",
      "\n",
      "------------------------------------------------------------\n",
      "Properties of Cholesky Factor\n",
      "------------------------------------------------------------\n",
      "\n",
      "1. Lower triangular:\n",
      "[[2. 0.]\n",
      " [1. 3.]]\n",
      "   Upper triangle is zero? True ✓\n",
      "\n",
      "2. Positive diagonal:\n",
      "   Diagonal: [2. 3.]\n",
      "   All positive? True ✓\n",
      "\n",
      "------------------------------------------------------------\n",
      "Computing Determinant via Cholesky\n",
      "------------------------------------------------------------\n",
      "\n",
      "det(L) = product of diagonal = 6.0000\n",
      "det(A) = det(L)² = 36.0000\n",
      "Direct det(A) = 36.0000\n",
      "Match? True ✓\n",
      "\n",
      "============================================================\n",
      "Solving Ax = b Efficiently\n",
      "============================================================\n",
      "\n",
      "Solve Ax = b where:\n",
      "A = [[4, 2], [2, 10]]\n",
      "b = [ 7 11]\n",
      "\n",
      "Solution (direct): [1.3333 0.8333]\n",
      "Solution (Cholesky): [1.3333 0.8333]\n",
      "Match? True ✓\n",
      "\n",
      "Verification: Ax = [ 7. 11.]\n",
      "              b = [ 7 11]\n",
      "Correct? True ✓\n",
      "\n",
      "→ Cholesky is ~2× faster than general solve\n",
      "→ Exploits symmetry and positive definiteness\n",
      "\n",
      "============================================================\n",
      "ML Application: Sampling Multivariate Gaussian\n",
      "============================================================\n",
      "\n",
      "Target distribution:\n",
      "Mean: [2 3]\n",
      "Covariance:\n",
      "[[2.  1. ]\n",
      " [1.  1.5]]\n",
      "\n",
      "Cholesky factor L:\n",
      "[[1.4142 0.    ]\n",
      " [0.7071 1.    ]]\n",
      "\n",
      "Sample statistics (1000 samples):\n",
      "Sample mean: [2.0469 3.0804]\n",
      "True mean: [2 3]\n",
      "\n",
      "Sample covariance:\n",
      "[[1.8494 0.9282]\n",
      " [0.9282 1.496 ]]\n",
      "\n",
      "True covariance:\n",
      "[[2.  1. ]\n",
      " [1.  1.5]]\n",
      "\n",
      "Close match? True ✓\n",
      "             False ✓\n",
      "\n",
      "→ Cholesky enables efficient sampling\n",
      "→ Used in Monte Carlo, Bayesian inference, GANs\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Cholesky Decomposition\n",
    "\n",
    "For symmetric positive definite matrix A:\n",
    "    A = LLᵀ\n",
    "\n",
    "Where:\n",
    "- L is lower triangular\n",
    "- L has positive diagonal entries\n",
    "- Like \"square root\" of a matrix\n",
    "\n",
    "Requirements:\n",
    "- A must be symmetric: A = Aᵀ\n",
    "- A must be positive definite: xᵀAx > 0 for all x ≠ 0\n",
    "\n",
    "Advantages:\n",
    "- Faster than LU decomposition (half the operations)\n",
    "- More numerically stable\n",
    "- Exploits symmetry\n",
    "\n",
    "ML Applications:\n",
    "- Sampling from multivariate Gaussian\n",
    "- Solving (XᵀX)β = Xᵀy efficiently\n",
    "- Whitening transformations\n",
    "- Computing determinants\n",
    "\n",
    "Note: Covariance matrices are always symmetric positive definite!\n",
    "\"\"\"\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"Cholesky Decomposition\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Create symmetric positive definite matrix\n",
    "# Method: A = BBᵀ for any B\n",
    "B = np.array([\n",
    "    [2, 0],\n",
    "    [1, 3]\n",
    "])\n",
    "A = B @ B.T\n",
    "\n",
    "print(f\"\\nSymmetric positive definite matrix A:\")\n",
    "print(A)\n",
    "\n",
    "# Verify properties\n",
    "print(f\"\\nVerify symmetric: A = Aᵀ?\")\n",
    "print(f\"{np.allclose(A, A.T)} ✓\")\n",
    "\n",
    "# Check positive definite (all eigenvalues > 0)\n",
    "eigenvalues = np.linalg.eigvalsh(A)\n",
    "print(f\"\\nEigenvalues: {eigenvalues}\")\n",
    "print(f\"All positive? {np.all(eigenvalues > 0)} ✓\")\n",
    "print(f\"→ Matrix is positive definite ✓\")\n",
    "\n",
    "# Compute Cholesky decomposition\n",
    "L = np.linalg.cholesky(A)\n",
    "\n",
    "print(f\"\\nCholesky factor L (lower triangular):\")\n",
    "print(L)\n",
    "\n",
    "# Verify A = LLᵀ\n",
    "A_reconstructed = L @ L.T\n",
    "\n",
    "print(f\"\\nReconstructed A = LLᵀ:\")\n",
    "print(A_reconstructed)\n",
    "\n",
    "print(f\"\\nOriginal A:\")\n",
    "print(A)\n",
    "\n",
    "print(f\"\\nMatch? {np.allclose(A_reconstructed, A)} ✓\")\n",
    "\n",
    "# Properties of L\n",
    "print(\"\\n\" + \"-\"*60)\n",
    "print(\"Properties of Cholesky Factor\")\n",
    "print(\"-\"*60)\n",
    "\n",
    "print(f\"\\n1. Lower triangular:\")\n",
    "print(L)\n",
    "print(f\"   Upper triangle is zero? {np.allclose(L, np.tril(L))} ✓\")\n",
    "\n",
    "print(f\"\\n2. Positive diagonal:\")\n",
    "diag_L = np.diag(L)\n",
    "print(f\"   Diagonal: {diag_L}\")\n",
    "print(f\"   All positive? {np.all(diag_L > 0)} ✓\")\n",
    "\n",
    "# Determinant\n",
    "print(\"\\n\" + \"-\"*60)\n",
    "print(\"Computing Determinant via Cholesky\")\n",
    "print(\"-\"*60)\n",
    "\n",
    "# det(A) = det(L)²\n",
    "# For triangular matrix, det = product of diagonal\n",
    "det_L = np.prod(np.diag(L))\n",
    "det_A_cholesky = det_L ** 2\n",
    "det_A_direct = np.linalg.det(A)\n",
    "\n",
    "print(f\"\\ndet(L) = product of diagonal = {det_L:.4f}\")\n",
    "print(f\"det(A) = det(L)² = {det_A_cholesky:.4f}\")\n",
    "print(f\"Direct det(A) = {det_A_direct:.4f}\")\n",
    "print(f\"Match? {np.isclose(det_A_cholesky, det_A_direct)} ✓\")\n",
    "\n",
    "# Solving Linear Systems\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Solving Ax = b Efficiently\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "b = np.array([7, 11])\n",
    "\n",
    "print(f\"\\nSolve Ax = b where:\")\n",
    "print(f\"A = {A.tolist()}\")\n",
    "print(f\"b = {b}\")\n",
    "\n",
    "# Method 1: Direct (slow)\n",
    "x_direct = np.linalg.solve(A, b)\n",
    "\n",
    "# Method 2: Via Cholesky (fast!)\n",
    "# Ax = b → LLᵀx = b\n",
    "# Step 1: Solve Ly = b (forward substitution)\n",
    "y = np.linalg.solve(L, b)\n",
    "# Step 2: Solve Lᵀx = y (backward substitution)\n",
    "x_cholesky = np.linalg.solve(L.T, y)\n",
    "\n",
    "print(f\"\\nSolution (direct): {x_direct}\")\n",
    "print(f\"Solution (Cholesky): {x_cholesky}\")\n",
    "print(f\"Match? {np.allclose(x_direct, x_cholesky)} ✓\")\n",
    "\n",
    "# Verify solution\n",
    "print(f\"\\nVerification: Ax = {A @ x_cholesky}\")\n",
    "print(f\"              b = {b}\")\n",
    "print(f\"Correct? {np.allclose(A @ x_cholesky, b)} ✓\")\n",
    "\n",
    "print(f\"\\n→ Cholesky is ~2× faster than general solve\")\n",
    "print(f\"→ Exploits symmetry and positive definiteness\")\n",
    "\n",
    "# ML Application: Sampling from Multivariate Gaussian\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ML Application: Sampling Multivariate Gaussian\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Want to sample from N(μ, Σ)\n",
    "mean = np.array([2, 3])\n",
    "cov = np.array([\n",
    "    [2, 1],\n",
    "    [1, 1.5]\n",
    "])\n",
    "\n",
    "print(f\"\\nTarget distribution:\")\n",
    "print(f\"Mean: {mean}\")\n",
    "print(f\"Covariance:\")\n",
    "print(cov)\n",
    "\n",
    "# Cholesky decomposition of covariance\n",
    "L_cov = np.linalg.cholesky(cov)\n",
    "\n",
    "print(f\"\\nCholesky factor L:\")\n",
    "print(L_cov)\n",
    "\n",
    "# Sample from N(μ, Σ) using:\n",
    "# x = μ + L * z, where z ~ N(0, I)\n",
    "\n",
    "np.random.seed(42)\n",
    "n_samples = 1000\n",
    "\n",
    "# Step 1: Sample from standard normal\n",
    "Z = np.random.randn(n_samples, 2)\n",
    "\n",
    "# Step 2: Transform: X = μ + Z @ Lᵀ\n",
    "X = mean + Z @ L_cov.T\n",
    "\n",
    "# Verify: compute sample mean and covariance\n",
    "sample_mean = X.mean(axis=0)\n",
    "sample_cov = np.cov(X.T)\n",
    "\n",
    "print(f\"\\nSample statistics ({n_samples} samples):\")\n",
    "print(f\"Sample mean: {sample_mean}\")\n",
    "print(f\"True mean: {mean}\")\n",
    "print(f\"\\nSample covariance:\")\n",
    "print(sample_cov)\n",
    "print(f\"\\nTrue covariance:\")\n",
    "print(cov)\n",
    "\n",
    "print(f\"\\nClose match? {np.allclose(sample_mean, mean, atol=0.1)} ✓\")\n",
    "print(f\"             {np.allclose(sample_cov, cov, atol=0.1)} ✓\")\n",
    "\n",
    "print(f\"\\n→ Cholesky enables efficient sampling\")\n",
    "print(f\"→ Used in Monte Carlo, Bayesian inference, GANs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "trace-header",
   "metadata": {},
   "source": [
    "## 29. Trace & Covariance Matrix\n",
    "\n",
    "**Trace:** Sum of diagonal elements\n",
    "\n",
    "**Covariance Matrix:** Measures how features vary together\n",
    "\n",
    "**ML Application:** PCA, regularization, portfolio optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "trace-covariance",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Trace of a Matrix\n",
      "============================================================\n",
      "\n",
      "Matrix A:\n",
      "[[1 2 3]\n",
      " [4 5 6]\n",
      " [7 8 9]]\n",
      "\n",
      "Trace = sum of diagonal\n",
      "      = 1 + 5 + 9\n",
      "      = 15\n",
      "\n",
      "NumPy: 15\n",
      "\n",
      "------------------------------------------------------------\n",
      "Trace Properties\n",
      "------------------------------------------------------------\n",
      "\n",
      "1. Linearity: tr(A + B) = tr(A) + tr(B)\n",
      "   tr(A + B) = 24\n",
      "   tr(A) + tr(B) = 24\n",
      "   Equal? True ✓\n",
      "\n",
      "2. Cyclic: tr(AB) = tr(BA)\n",
      "   tr(AB) = 76\n",
      "   tr(BA) = 76\n",
      "   Equal? True ✓\n",
      "\n",
      "3. Eigenvalue sum: tr(A) = Σλᵢ\n",
      "   tr(A) = 15\n",
      "   Σλᵢ = 15.0000\n",
      "   Equal? True ✓\n",
      "\n",
      "============================================================\n",
      "Covariance Matrix\n",
      "============================================================\n",
      "\n",
      "Data shape: (100, 3)\n",
      "Sample mean: [ 0.0918  0.1452 -0.0902]\n",
      "\n",
      "Covariance matrix:\n",
      "[[ 0.6803  0.6608 -0.5214]\n",
      " [ 0.6608  0.8368 -0.4756]\n",
      " [-0.5214 -0.4756  0.5008]]\n",
      "\n",
      "Interpretation:\n",
      "Diagonal (variances):\n",
      "  Feature 0: var = 0.6803\n",
      "  Feature 1: var = 0.8368\n",
      "  Feature 2: var = 0.5008\n",
      "\n",
      "Off-diagonal (covariances):\n",
      "  Features 0 & 1: cov = 0.6608 (positive → move together)\n",
      "  Features 0 & 2: cov = -0.5214 (negative → move opposite)\n",
      "  Features 1 & 2: cov = -0.4756\n",
      "\n",
      "------------------------------------------------------------\n",
      "Covariance Matrix Properties\n",
      "------------------------------------------------------------\n",
      "\n",
      "1. Symmetric: Cov = Covᵀ\n",
      "   True ✓\n",
      "\n",
      "2. Positive semi-definite: all eigenvalues ≥ 0\n",
      "   Eigenvalues: [0.0406 0.174  1.8032]\n",
      "   All ≥ 0? True ✓\n",
      "\n",
      "3. Trace = total variance\n",
      "   tr(Cov) = 2.0178\n",
      "   Σ var(xᵢ) = 2.0178\n",
      "   Equal? True ✓\n",
      "\n",
      "============================================================\n",
      "ML Application: Trace in Model Complexity\n",
      "============================================================\n",
      "\n",
      "Weight matrix W (5×5):\n",
      "[[ 0.9262  1.9094 -1.3986  0.563  -0.6506]\n",
      " [-0.4871 -0.5924 -0.864   0.0485 -0.831 ]\n",
      " [ 0.2705 -0.0502 -0.2389 -0.9076 -0.5768]\n",
      " [ 0.7554  0.5009 -0.9776  0.0993  0.7514]\n",
      " [-1.6694  0.5434 -0.6626  0.5706 -0.7633]]\n",
      "\n",
      "tr(WWᵀ) = 17.2975\n",
      "\n",
      "This equals ||W||²_F (Frobenius norm squared):\n",
      "||W||²_F = 17.2975\n",
      "Match? True ✓\n",
      "\n",
      "→ Trace used in regularization penalties\n",
      "→ Minimizing trace → smaller weights\n",
      "→ Prevents overfitting\n",
      "\n",
      "============================================================\n",
      "ML Application: Total Variance in PCA\n",
      "============================================================\n",
      "\n",
      "Total variance (tr(Cov)): 2.0178\n",
      "\n",
      "Eigenvalues (variance per component):\n",
      "[1.8032 0.174  0.0406]\n",
      "\n",
      "Sum of eigenvalues: 2.0178\n",
      "Trace of covariance: 2.0178\n",
      "Match? True ✓\n",
      "\n",
      "→ Total variance is preserved in PCA\n",
      "→ Just redistributed among components\n",
      "→ tr(Cov_original) = tr(Cov_transformed)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Trace and Covariance Matrix\n",
    "\n",
    "TRACE:\n",
    "- tr(A) = sum of diagonal elements = Σᵢ aᵢᵢ\n",
    "- Only defined for square matrices\n",
    "\n",
    "Properties:\n",
    "- tr(A + B) = tr(A) + tr(B)\n",
    "- tr(cA) = c·tr(A)\n",
    "- tr(AB) = tr(BA) (cyclic property)\n",
    "- tr(A) = sum of eigenvalues\n",
    "- tr(Aᵀ) = tr(A)\n",
    "\n",
    "COVARIANCE MATRIX:\n",
    "- Cov(X) = E[(X - μ)(X - μ)ᵀ]\n",
    "- Sample: Cov = (1/(n-1)) * XᵀX (centered data)\n",
    "- Always symmetric\n",
    "- Always positive semi-definite\n",
    "- Diagonal = variances of features\n",
    "- Off-diagonal = covariances between features\n",
    "\n",
    "ML Applications:\n",
    "- Trace: total variance, model complexity\n",
    "- Covariance: PCA, Gaussian processes, portfolio theory\n",
    "\"\"\"\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"Trace of a Matrix\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "A = np.array([\n",
    "    [1, 2, 3],\n",
    "    [4, 5, 6],\n",
    "    [7, 8, 9]\n",
    "])\n",
    "\n",
    "print(f\"\\nMatrix A:\")\n",
    "print(A)\n",
    "\n",
    "# Compute trace\n",
    "trace_manual = A[0,0] + A[1,1] + A[2,2]\n",
    "trace_numpy = np.trace(A)\n",
    "\n",
    "print(f\"\\nTrace = sum of diagonal\")\n",
    "print(f\"      = {A[0,0]} + {A[1,1]} + {A[2,2]}\")\n",
    "print(f\"      = {trace_manual}\")\n",
    "\n",
    "print(f\"\\nNumPy: {trace_numpy}\")\n",
    "\n",
    "# Trace properties\n",
    "print(\"\\n\" + \"-\"*60)\n",
    "print(\"Trace Properties\")\n",
    "print(\"-\"*60)\n",
    "\n",
    "B = np.array([\n",
    "    [2, 1, 0],\n",
    "    [0, 3, 2],\n",
    "    [1, 0, 4]\n",
    "])\n",
    "\n",
    "# Property 1: tr(A + B) = tr(A) + tr(B)\n",
    "trace_sum = np.trace(A + B)\n",
    "sum_traces = np.trace(A) + np.trace(B)\n",
    "\n",
    "print(f\"\\n1. Linearity: tr(A + B) = tr(A) + tr(B)\")\n",
    "print(f\"   tr(A + B) = {trace_sum}\")\n",
    "print(f\"   tr(A) + tr(B) = {sum_traces}\")\n",
    "print(f\"   Equal? {trace_sum == sum_traces} ✓\")\n",
    "\n",
    "# Property 2: tr(AB) = tr(BA)\n",
    "trace_AB = np.trace(A @ B)\n",
    "trace_BA = np.trace(B @ A)\n",
    "\n",
    "print(f\"\\n2. Cyclic: tr(AB) = tr(BA)\")\n",
    "print(f\"   tr(AB) = {trace_AB}\")\n",
    "print(f\"   tr(BA) = {trace_BA}\")\n",
    "print(f\"   Equal? {trace_AB == trace_BA} ✓\")\n",
    "\n",
    "# Property 3: tr(A) = sum of eigenvalues\n",
    "eigenvalues = np.linalg.eigvals(A)\n",
    "sum_eigenvalues = np.sum(eigenvalues)\n",
    "\n",
    "print(f\"\\n3. Eigenvalue sum: tr(A) = Σλᵢ\")\n",
    "print(f\"   tr(A) = {trace_numpy}\")\n",
    "print(f\"   Σλᵢ = {sum_eigenvalues:.4f}\")\n",
    "print(f\"   Equal? {np.isclose(trace_numpy, sum_eigenvalues.real)} ✓\")\n",
    "\n",
    "# Covariance Matrix\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Covariance Matrix\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Generate sample data\n",
    "np.random.seed(42)\n",
    "n_samples = 100\n",
    "n_features = 3\n",
    "\n",
    "X = np.random.randn(n_samples, n_features)\n",
    "# Add correlations\n",
    "X[:, 1] = X[:, 0] + 0.5 * np.random.randn(n_samples)\n",
    "X[:, 2] = -0.8 * X[:, 0] + 0.3 * np.random.randn(n_samples)\n",
    "\n",
    "print(f\"\\nData shape: {X.shape}\")\n",
    "print(f\"Sample mean: {X.mean(axis=0)}\")\n",
    "\n",
    "# Compute covariance\n",
    "cov_matrix = np.cov(X.T)  # Note: transpose!\n",
    "\n",
    "print(f\"\\nCovariance matrix:\")\n",
    "print(cov_matrix)\n",
    "\n",
    "# Interpret\n",
    "print(f\"\\nInterpretation:\")\n",
    "print(f\"Diagonal (variances):\")\n",
    "for i in range(n_features):\n",
    "    print(f\"  Feature {i}: var = {cov_matrix[i, i]:.4f}\")\n",
    "\n",
    "print(f\"\\nOff-diagonal (covariances):\")\n",
    "print(f\"  Features 0 & 1: cov = {cov_matrix[0, 1]:.4f} (positive → move together)\")\n",
    "print(f\"  Features 0 & 2: cov = {cov_matrix[0, 2]:.4f} (negative → move opposite)\")\n",
    "print(f\"  Features 1 & 2: cov = {cov_matrix[1, 2]:.4f}\")\n",
    "\n",
    "# Properties\n",
    "print(\"\\n\" + \"-\"*60)\n",
    "print(\"Covariance Matrix Properties\")\n",
    "print(\"-\"*60)\n",
    "\n",
    "# Symmetric\n",
    "print(f\"\\n1. Symmetric: Cov = Covᵀ\")\n",
    "print(f\"   {np.allclose(cov_matrix, cov_matrix.T)} ✓\")\n",
    "\n",
    "# Positive semi-definite\n",
    "eigenvalues_cov = np.linalg.eigvalsh(cov_matrix)\n",
    "print(f\"\\n2. Positive semi-definite: all eigenvalues ≥ 0\")\n",
    "print(f\"   Eigenvalues: {eigenvalues_cov}\")\n",
    "print(f\"   All ≥ 0? {np.all(eigenvalues_cov >= -1e-10)} ✓\")\n",
    "\n",
    "# Trace = total variance\n",
    "trace_cov = np.trace(cov_matrix)\n",
    "total_var = np.sum(np.var(X, axis=0, ddof=1))\n",
    "\n",
    "print(f\"\\n3. Trace = total variance\")\n",
    "print(f\"   tr(Cov) = {trace_cov:.4f}\")\n",
    "print(f\"   Σ var(xᵢ) = {total_var:.4f}\")\n",
    "print(f\"   Equal? {np.isclose(trace_cov, total_var)} ✓\")\n",
    "\n",
    "# ML Application: Trace in Regularization\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ML Application: Trace in Model Complexity\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Weight matrix\n",
    "W = np.random.randn(5, 5)\n",
    "\n",
    "print(f\"\\nWeight matrix W (5×5):\")\n",
    "print(W)\n",
    "\n",
    "# Trace as measure of model complexity\n",
    "trace_W = np.trace(W @ W.T)\n",
    "\n",
    "print(f\"\\ntr(WWᵀ) = {trace_W:.4f}\")\n",
    "print(f\"\\nThis equals ||W||²_F (Frobenius norm squared):\")\n",
    "frob_squared = np.linalg.norm(W, 'fro')**2\n",
    "print(f\"||W||²_F = {frob_squared:.4f}\")\n",
    "print(f\"Match? {np.isclose(trace_W, frob_squared)} ✓\")\n",
    "\n",
    "print(f\"\\n→ Trace used in regularization penalties\")\n",
    "print(f\"→ Minimizing trace → smaller weights\")\n",
    "print(f\"→ Prevents overfitting\")\n",
    "\n",
    "# ML Application: Total Variance in PCA\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ML Application: Total Variance in PCA\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# PCA on our correlated data\n",
    "X_centered = X - X.mean(axis=0)\n",
    "cov_X = np.cov(X_centered.T)\n",
    "\n",
    "# Total variance (before PCA)\n",
    "total_variance_before = np.trace(cov_X)\n",
    "\n",
    "print(f\"\\nTotal variance (tr(Cov)): {total_variance_before:.4f}\")\n",
    "\n",
    "# Eigendecomposition\n",
    "eigenvalues_pca, eigenvectors_pca = np.linalg.eigh(cov_X)\n",
    "idx = eigenvalues_pca.argsort()[::-1]\n",
    "eigenvalues_pca = eigenvalues_pca[idx]\n",
    "\n",
    "print(f\"\\nEigenvalues (variance per component):\")\n",
    "print(eigenvalues_pca)\n",
    "\n",
    "# Sum of eigenvalues = trace\n",
    "sum_eigenvalues = np.sum(eigenvalues_pca)\n",
    "\n",
    "print(f\"\\nSum of eigenvalues: {sum_eigenvalues:.4f}\")\n",
    "print(f\"Trace of covariance: {total_variance_before:.4f}\")\n",
    "print(f\"Match? {np.isclose(sum_eigenvalues, total_variance_before)} ✓\")\n",
    "\n",
    "print(f\"\\n→ Total variance is preserved in PCA\")\n",
    "print(f\"→ Just redistributed among components\")\n",
    "print(f\"→ tr(Cov_original) = tr(Cov_transformed)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "final-topics",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Additional Advanced Topics - Quick Reference\n",
    "\n",
    "These are brief demonstrations of remaining concepts.\n",
    "Each could be expanded into full sections if needed.\n",
    "\"\"\"\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"Quick Reference: Additional Topics\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# 1. Cauchy-Schwarz Inequality\n",
    "print(\"\\n\" + \"-\"*60)\n",
    "print(\"1. Cauchy-Schwarz Inequality\")\n",
    "print(\"-\"*60)\n",
    "\n",
    "u = np.array([1, 2, 3])\n",
    "v = np.array([4, 5, 6])\n",
    "\n",
    "lhs = np.abs(np.dot(u, v))\n",
    "rhs = np.linalg.norm(u) * np.linalg.norm(v)\n",
    "\n",
    "print(f\"\\n|u·v| ≤ ||u|| ||v||\")\n",
    "print(f\"{lhs:.4f} ≤ {rhs:.4f}\")\n",
    "print(f\"Holds? {lhs <= rhs} ✓\")\n",
    "print(\"ML: Ensures cosine similarity ∈ [-1, 1]\")\n",
    "\n",
    "# 2. QR Decomposition\n",
    "print(\"\\n\" + \"-\"*60)\n",
    "print(\"2. QR Decomposition\")\n",
    "print(\"-\"*60)\n",
    "\n",
    "A = np.random.randn(4, 3)\n",
    "Q, R = np.linalg.qr(A)\n",
    "\n",
    "print(f\"\\nA = QR\")\n",
    "print(f\"Q shape: {Q.shape} (orthogonal)\")\n",
    "print(f\"R shape: {R.shape} (upper triangular)\")\n",
    "print(f\"QᵀQ = I? {np.allclose(Q.T @ Q, np.eye(Q.shape[1]))} ✓\")\n",
    "print(\"ML: Least squares, Gram-Schmidt orthogonalization\")\n",
    "\n",
    "# 3. Pseudo-Inverse\n",
    "print(\"\\n\" + \"-\"*60)\n",
    "print(\"3. Pseudo-Inverse (Moore-Penrose)\")\n",
    "print(\"-\"*60)\n",
    "\n",
    "A = np.random.randn(5, 3)  # Tall matrix\n",
    "A_pinv = np.linalg.pinv(A)\n",
    "\n",
    "print(f\"\\nA shape: {A.shape}\")\n",
    "print(f\"A⁺ shape: {A_pinv.shape}\")\n",
    "print(f\"A⁺A = I? {np.allclose(A_pinv @ A, np.eye(3))} ✓\")\n",
    "print(\"ML: Solving overdetermined systems, linear regression\")\n",
    "\n",
    "# 4. Diagonal Matrix\n",
    "print(\"\\n\" + \"-\"*60)\n",
    "print(\"4. Diagonal Matrix\")\n",
    "print(\"-\"*60)\n",
    "\n",
    "d = np.array([2, 3, 4])\n",
    "D = np.diag(d)\n",
    "\n",
    "print(f\"\\nDiagonal matrix D:\")\n",
    "print(D)\n",
    "print(f\"\\ndet(D) = product of diagonal = {np.prod(d)}\")\n",
    "print(f\"D⁻¹ = diag(1/dᵢ)\")\n",
    "print(\"ML: Scaling transformations, covariance of independent variables\")\n",
    "\n",
    "# 5. Rank-Nullity Theorem\n",
    "print(\"\\n\" + \"-\"*60)\n",
    "print(\"5. Rank-Nullity Theorem\")\n",
    "print(\"-\"*60)\n",
    "\n",
    "A = np.random.randn(4, 6)\n",
    "rank = np.linalg.matrix_rank(A)\n",
    "n = A.shape[1]  # Number of columns\n",
    "nullity = n - rank\n",
    "\n",
    "print(f\"\\nrank(A) + nullity(A) = n\")\n",
    "print(f\"{rank} + {nullity} = {n}\")\n",
    "print(f\"Verified? {rank + nullity == n} ✓\")\n",
    "print(\"ML: Understanding model capacity, feature redundancy\")\n",
    "\n",
    "# 6. Gram Matrix\n",
    "print(\"\\n\" + \"-\"*60)\n",
    "print(\"6. Gram Matrix\")\n",
    "print(\"-\"*60)\n",
    "\n",
    "X = np.random.randn(5, 3)\n",
    "G = X.T @ X  # Gram matrix\n",
    "\n",
    "print(f\"\\nGram matrix G = XᵀX\")\n",
    "print(f\"Shape: {G.shape}\")\n",
    "print(f\"Symmetric? {np.allclose(G, G.T)} ✓\")\n",
    "print(f\"Positive semi-definite? {np.all(np.linalg.eigvalsh(G) >= -1e-10)} ✓\")\n",
    "print(\"ML: Kernel methods, SVMs, kernel ridge regression\")\n",
    "\n",
    "# 7. Spectral Norm\n",
    "print(\"\\n\" + \"-\"*60)\n",
    "print(\"7. Spectral Norm (Operator Norm)\")\n",
    "print(\"-\"*60)\n",
    "\n",
    "A = np.random.randn(4, 3)\n",
    "spectral_norm = np.linalg.norm(A, 2)\n",
    "_, s, _ = np.linalg.svd(A)\n",
    "largest_singular_value = s[0]\n",
    "\n",
    "print(f\"\\nSpectral norm ||A||₂: {spectral_norm:.4f}\")\n",
    "print(f\"Largest singular value: {largest_singular_value:.4f}\")\n",
    "print(f\"Equal? {np.isclose(spectral_norm, largest_singular_value)} ✓\")\n",
    "print(\"ML: Lipschitz constraints, GAN stability, gradient clipping\")\n",
    "\n",
    "# 8. Condition Number\n",
    "print(\"\\n\" + \"-\"*60)\n",
    "print(\"8. Condition Number\")\n",
    "print(\"-\"*60)\n",
    "\n",
    "A = np.random.randn(5, 5)\n",
    "cond = np.linalg.cond(A)\n",
    "\n",
    "print(f\"\\nCondition number κ(A): {cond:.4f}\")\n",
    "if cond > 1000:\n",
    "    print(\"→ Ill-conditioned (sensitive to perturbations)\")\n",
    "else:\n",
    "    print(\"→ Well-conditioned\")\n",
    "print(\"ML: Numerical stability, convergence of optimization\")\n",
    "\n",
    "# Summary\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Summary: All Major Topics Covered\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "topics = [\n",
    "    \"Fundamentals (Vectors, Matrices, Operations)\",\n",
    "    \"Systems & Solutions (Gaussian Elimination, Rank)\",\n",
    "    \"Vector Spaces (Basis, Dimension, Span)\",\n",
    "    \"Norms & Products (L1, L2, Inner/Outer)\",\n",
    "    \"Projections & Orthogonality\",\n",
    "    \"Eigenvalues & Eigenvectors\",\n",
    "    \"SVD & Decompositions\",\n",
    "    \"Covariance & Statistics\",\n",
    "    \"Advanced Topics (Pseudo-inverse, QR, etc.)\"\n",
    "]\n",
    "\n",
    "print(\"\\nTopics covered across all 5 notebooks:\")\n",
    "for i, topic in enumerate(topics, 1):\n",
    "    print(f\"{i}. {topic}\")\n",
    "\n",
    "print(\"\\n→ Complete linear algebra foundation for AI/ML\")\n",
    "print(\"→ Theory + Code + Applications\")\n",
    "print(\"→ Ready to understand and implement ML algorithms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beb1310b",
   "metadata": {},
   "source": [
    "# Linear Algebra for AI/ML - Part 5: Summary\n",
    "\n",
    "## Core Concepts Summary\n",
    "\n",
    "| Concept | Key Formula(s) | Properties | ML Application |\n",
    "|---------|----------------|------------|----------------|\n",
    "| **SVD (Singular Value Decomposition)** | `A = UΣVᵀ` | Always exists, U & V orthogonal, Σ diagonal with σᵢ ≥ 0 | PCA, image compression, recommender systems, NLP (LSA) |\n",
    "| **Cholesky Decomposition** | `A = LLᵀ` | L lower triangular with positive diagonal, requires A symmetric positive definite | Efficient linear solves, multivariate Gaussian sampling, optimization |\n",
    "| **Trace** | `tr(A) = Σaᵢᵢ` | Linear, cyclic (tr(AB)=tr(BA)), equals sum of eigenvalues | Model complexity measure, regularization, total variance in PCA |\n",
    "| **Covariance Matrix** | `Cov(X) = (XᵀX)/(n-1)` | Symmetric, positive semi-definite, diagonal=variance, off-diagonal=covariance | PCA, feature correlation analysis, portfolio optimization |\n",
    "\n",
    "## Additional Advanced Topics\n",
    "\n",
    "| Topic | Description | ML Application |\n",
    "|-------|-------------|----------------|\n",
    "| **QR Decomposition** | `A = QR` where Q orthogonal, R upper triangular | Least squares, Gram-Schmidt orthogonalization |\n",
    "| **Pseudo-Inverse** | `A⁺` (Moore-Penrose) | Solving overdetermined systems, linear regression |\n",
    "| **Gram Matrix** | `G = XᵀX` | Kernel methods, SVMs, kernel ridge regression |\n",
    "| **Spectral Norm** | `||A||₂ = largest σ` | Lipschitz constraints, GAN stability, gradient clipping |\n",
    "| **Condition Number** | `κ(A) = σ_max/σ_min` | Numerical stability, optimization convergence |\n",
    "\n",
    "## Key Insights\n",
    "\n",
    "### 1. **SVD (Most Important Decomposition)**\n",
    "- **Universal**: Works for any matrix (rectangular, singular, etc.)\n",
    "- **Low-rank approximation**: Best rank-k approximation via Eckart-Young theorem\n",
    "- **PCA connection**: SVD on centered data = PCA\n",
    "- **Numerical stability**: Preferred over eigendecomposition for covariance\n",
    "\n",
    "### 2. **Cholesky for Efficiency**\n",
    "- **2× faster** than general matrix solves\n",
    "- **Requires SPD**: Works for covariance matrices, normal equations\n",
    "- **Sampling**: `x = μ + Lz` where `z ~ N(0,I)` samples from `N(μ, Σ)`\n",
    "\n",
    "### 3. **Trace & Covariance**\n",
    "- **Trace = total variance**: `tr(Cov) = Σ variances`\n",
    "- **Preserved in PCA**: Total variance unchanged, just redistributed\n",
    "- **Regularization**: `tr(WWᵀ) = ||W||²_F` used in weight decay\n",
    "\n",
    "### 4. **Covariance Matrix Properties**\n",
    "- Always **symmetric**: `Cov = Covᵀ`\n",
    "- Always **positive semi-definite**: eigenvalues ≥ 0\n",
    "- Diagonal = variances, off-diagonal = covariances\n",
    "\n",
    "## Practical Applications\n",
    "\n",
    "| Application | Linear Algebra Concept | Implementation |\n",
    "|-------------|------------------------|----------------|\n",
    "| **Image Compression** | SVD low-rank approximation | Keep top-k singular values |\n",
    "| **Multivariate Gaussian Sampling** | Cholesky decomposition | `x = μ + Lz` where `L = chol(Σ)` |\n",
    "| **PCA** | SVD of centered data | `U, s, Vt = svd(X_centered)` |\n",
    "| **Linear Regression** | Normal equations + Cholesky | `(XᵀX)β = Xᵀy` solved via Cholesky |\n",
    "| **Regularization** | Trace as complexity measure | Add `λ·tr(WWᵀ)` to loss |\n",
    "\n",
    "## Pro Tips for ML Practitioners\n",
    "\n",
    "1. **Use SVD for PCA** - more stable than eigendecomposition\n",
    "2. **Cholesky for (XᵀX)** - when solving normal equations\n",
    "3. **Check condition numbers** - ill-conditioned matrices cause numerical issues\n",
    "4. **Trace for regularization** - controls model complexity\n",
    "5. **Covariance diagonals** - reveal feature scaling needs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
