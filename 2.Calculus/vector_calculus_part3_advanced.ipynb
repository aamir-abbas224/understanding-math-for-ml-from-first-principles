{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# Vector Calculus for AI/ML - Part 3: Taylor Series, Hessian & Advanced Topics\n",
    "\n",
    "This final notebook covers advanced calculus concepts essential for optimization.\n",
    "\n",
    "**Key Topics:** Taylor Series, Hessian Matrix, Linear Approximation, Optimization Landscape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "setup",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ready for advanced calculus!\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Setup: Import Required Libraries\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sympy as sp\n",
    "from scipy.optimize import minimize\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "np.set_printoptions(precision=4, suppress=True)\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "\n",
    "print(\"Ready for advanced calculus!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "taylor-header",
   "metadata": {},
   "source": [
    "## 10. Taylor Series\n",
    "\n",
    "**Approximate any function using derivatives at a point.**\n",
    "\n",
    "**Formula:**\n",
    "$$f(x) \\approx f(a) + f'(a)(x-a) + \\frac{f''(a)}{2!}(x-a)^2 + \\frac{f'''(a)}{3!}(x-a)^3 + \\cdots$$\n",
    "\n",
    "**Orders:**\n",
    "- 0th order: f(a) (constant)\n",
    "- 1st order: f(a) + f'(a)(x-a) (linear)\n",
    "- 2nd order: + quadratic term (captures curvature)\n",
    "\n",
    "**ML Applications:**\n",
    "- Gradient descent (1st order)\n",
    "- Newton's method (2nd order)\n",
    "- Trust region methods\n",
    "- Local approximations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "taylor",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Taylor Series Approximation\n",
      "============================================================\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "x and y must have same first dimension, but have shapes (200,) and (1,)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 56\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;66;03m# Plot function and approximations\u001b[39;00m\n\u001b[0;32m     55\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(x_plot, f(x_plot), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mk-\u001b[39m\u001b[38;5;124m'\u001b[39m, linewidth\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m, label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mf(x) = e^x (exact)\u001b[39m\u001b[38;5;124m'\u001b[39m, zorder\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m)\n\u001b[1;32m---> 56\u001b[0m \u001b[43mplt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mplot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_plot\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtaylor_0th\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_plot\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ma_expand\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mb--\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlinewidth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m0th order (constant)\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malpha\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.7\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     57\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(x_plot, taylor_1st(x_plot, a_expand), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mg--\u001b[39m\u001b[38;5;124m'\u001b[39m, linewidth\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m1st order (linear)\u001b[39m\u001b[38;5;124m'\u001b[39m, alpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.7\u001b[39m)\n\u001b[0;32m     58\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(x_plot, taylor_2nd(x_plot, a_expand), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr--\u001b[39m\u001b[38;5;124m'\u001b[39m, linewidth\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m2nd order (quadratic)\u001b[39m\u001b[38;5;124m'\u001b[39m, alpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.7\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Aamir Bin Abbas\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\matplotlib\\pyplot.py:3838\u001b[0m, in \u001b[0;36mplot\u001b[1;34m(scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[0;32m   3830\u001b[0m \u001b[38;5;129m@_copy_docstring_and_deprecators\u001b[39m(Axes\u001b[38;5;241m.\u001b[39mplot)\n\u001b[0;32m   3831\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mplot\u001b[39m(\n\u001b[0;32m   3832\u001b[0m     \u001b[38;5;241m*\u001b[39margs: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m ArrayLike \u001b[38;5;241m|\u001b[39m \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3836\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   3837\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mlist\u001b[39m[Line2D]:\n\u001b[1;32m-> 3838\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgca\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mplot\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   3839\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3840\u001b[0m \u001b[43m        \u001b[49m\u001b[43mscalex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscalex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3841\u001b[0m \u001b[43m        \u001b[49m\u001b[43mscaley\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscaley\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3842\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m}\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3843\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3844\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Aamir Bin Abbas\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\matplotlib\\axes\\_axes.py:1777\u001b[0m, in \u001b[0;36mAxes.plot\u001b[1;34m(self, scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1534\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1535\u001b[0m \u001b[38;5;124;03mPlot y versus x as lines and/or markers.\u001b[39;00m\n\u001b[0;32m   1536\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1774\u001b[0m \u001b[38;5;124;03m(``'green'``) or hex strings (``'#008000'``).\u001b[39;00m\n\u001b[0;32m   1775\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1776\u001b[0m kwargs \u001b[38;5;241m=\u001b[39m cbook\u001b[38;5;241m.\u001b[39mnormalize_kwargs(kwargs, mlines\u001b[38;5;241m.\u001b[39mLine2D)\n\u001b[1;32m-> 1777\u001b[0m lines \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_lines(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, data\u001b[38;5;241m=\u001b[39mdata, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)]\n\u001b[0;32m   1778\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m lines:\n\u001b[0;32m   1779\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd_line(line)\n",
      "File \u001b[1;32mc:\\Users\\Aamir Bin Abbas\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\matplotlib\\axes\\_base.py:297\u001b[0m, in \u001b[0;36m_process_plot_var_args.__call__\u001b[1;34m(self, axes, data, return_kwargs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    295\u001b[0m     this \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m args[\u001b[38;5;241m0\u001b[39m],\n\u001b[0;32m    296\u001b[0m     args \u001b[38;5;241m=\u001b[39m args[\u001b[38;5;241m1\u001b[39m:]\n\u001b[1;32m--> 297\u001b[0m \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_plot_args\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    298\u001b[0m \u001b[43m    \u001b[49m\u001b[43maxes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mthis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mambiguous_fmt_datakey\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mambiguous_fmt_datakey\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    299\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_kwargs\u001b[49m\n\u001b[0;32m    300\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Aamir Bin Abbas\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\matplotlib\\axes\\_base.py:494\u001b[0m, in \u001b[0;36m_process_plot_var_args._plot_args\u001b[1;34m(self, axes, tup, kwargs, return_kwargs, ambiguous_fmt_datakey)\u001b[0m\n\u001b[0;32m    491\u001b[0m     axes\u001b[38;5;241m.\u001b[39myaxis\u001b[38;5;241m.\u001b[39mupdate_units(y)\n\u001b[0;32m    493\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m!=\u001b[39m y\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]:\n\u001b[1;32m--> 494\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx and y must have same first dimension, but \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    495\u001b[0m                      \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhave shapes \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and \u001b[39m\u001b[38;5;132;01m{\u001b[39;00my\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    496\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m x\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m y\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[0;32m    497\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx and y can be no greater than 2D, but have \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    498\u001b[0m                      \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshapes \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and \u001b[39m\u001b[38;5;132;01m{\u001b[39;00my\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: x and y must have same first dimension, but have shapes (200,) and (1,)"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Taylor Series - Function Approximation\n",
    "\n",
    "Idea: Approximate complex function using polynomials\n",
    "\n",
    "Expansion around point a:\n",
    "f(x) = Σ [f^(n)(a) / n!] (x-a)^n\n",
    "\n",
    "Truncated (practical):\n",
    "- 1st order: f(x) ≈ f(a) + f'(a)(x-a)\n",
    "- 2nd order: f(x) ≈ f(a) + f'(a)(x-a) + (1/2)f''(a)(x-a)²\n",
    "\"\"\"\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"Taylor Series Approximation\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Example function: f(x) = e^x\n",
    "def f(x):\n",
    "    \"\"\"Exponential function\"\"\"\n",
    "    return np.exp(x)\n",
    "\n",
    "# Taylor approximations around a = 0\n",
    "def taylor_0th(x, a=0):\n",
    "    \"\"\"0th order: just constant f(a)\"\"\"\n",
    "    return f(a)\n",
    "\n",
    "def taylor_1st(x, a=0):\n",
    "    \"\"\"1st order: f(a) + f'(a)(x-a)\"\"\"\n",
    "    # For e^x: f'(x) = e^x\n",
    "    return f(a) + f(a)*(x - a)\n",
    "\n",
    "def taylor_2nd(x, a=0):\n",
    "    \"\"\"2nd order: + (1/2)f''(a)(x-a)²\"\"\"\n",
    "    # For e^x: f''(x) = e^x\n",
    "    return f(a) + f(a)*(x - a) + 0.5*f(a)*(x - a)**2\n",
    "\n",
    "def taylor_3rd(x, a=0):\n",
    "    \"\"\"3rd order: + (1/6)f'''(a)(x-a)³\"\"\"\n",
    "    return f(a) + f(a)*(x - a) + 0.5*f(a)*(x - a)**2 + (1/6)*f(a)*(x - a)**3\n",
    "\n",
    "def taylor_5th(x, a=0):\n",
    "    \"\"\"5th order Taylor approximation\"\"\"\n",
    "    fa = f(a)\n",
    "    dx = x - a\n",
    "    return fa * (1 + dx + dx**2/2 + dx**3/6 + dx**4/24 + dx**5/120)\n",
    "\n",
    "# Visualize approximations\n",
    "x_plot = np.linspace(-2, 2, 200)\n",
    "a_expand = 0  # Expansion point\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Plot function and approximations\n",
    "plt.plot(x_plot, f(x_plot), 'k-', linewidth=3, label='f(x) = e^x (exact)', zorder=10)\n",
    "plt.plot(x_plot, taylor_0th(x_plot, a_expand), 'b--', linewidth=2, label='0th order (constant)', alpha=0.7)\n",
    "plt.plot(x_plot, taylor_1st(x_plot, a_expand), 'g--', linewidth=2, label='1st order (linear)', alpha=0.7)\n",
    "plt.plot(x_plot, taylor_2nd(x_plot, a_expand), 'r--', linewidth=2, label='2nd order (quadratic)', alpha=0.7)\n",
    "plt.plot(x_plot, taylor_3rd(x_plot, a_expand), 'm--', linewidth=2, label='3rd order (cubic)', alpha=0.7)\n",
    "plt.plot(x_plot, taylor_5th(x_plot, a_expand), 'c--', linewidth=2, label='5th order', alpha=0.7)\n",
    "\n",
    "# Mark expansion point\n",
    "plt.plot(a_expand, f(a_expand), 'ro', markersize=12, label=f'Expansion point a={a_expand}', zorder=15)\n",
    "\n",
    "plt.xlabel('x', fontsize=12)\n",
    "plt.ylabel('y', fontsize=12)\n",
    "plt.title('Taylor Series Approximations of e^x around x=0', fontsize=14)\n",
    "plt.legend(loc='upper left')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.ylim(-0.5, 7)\n",
    "plt.axhline(y=0, color='k', linewidth=0.5)\n",
    "plt.axvline(x=0, color='k', linewidth=0.5)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Numerical comparison\n",
    "print(\"\\nApproximation quality at different points:\")\n",
    "print(\"-\"*70)\n",
    "print(\"   x     Exact    0th     1st     2nd     3rd     5th\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "test_points = [-1.0, -0.5, 0.0, 0.5, 1.0, 1.5]\n",
    "\n",
    "for x in test_points:\n",
    "    exact = f(x)\n",
    "    t0 = taylor_0th(x, a_expand)\n",
    "    t1 = taylor_1st(x, a_expand)\n",
    "    t2 = taylor_2nd(x, a_expand)\n",
    "    t3 = taylor_3rd(x, a_expand)\n",
    "    t5 = taylor_5th(x, a_expand)\n",
    "    \n",
    "    print(f\"{x:5.1f}  {exact:7.3f}  {t0:6.3f}  {t1:6.3f}  {t2:6.3f}  {t3:6.3f}  {t5:6.3f}\")\n",
    "\n",
    "print(\"\\n→ Higher order = better approximation\")\n",
    "print(\"→ But only NEAR the expansion point\")\n",
    "print(\"→ Further away → approximation breaks down\")\n",
    "\n",
    "# ML Application: Gradient Descent vs Newton's Method\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ML Application: Taylor Series in Optimization\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Define a loss function\n",
    "def loss(x):\n",
    "    \"\"\"Example loss: (x - 3)² + 1\"\"\"\n",
    "    return (x - 3)**2 + 1\n",
    "\n",
    "def loss_grad(x):\n",
    "    \"\"\"Gradient: 2(x - 3)\"\"\"\n",
    "    return 2*(x - 3)\n",
    "\n",
    "def loss_hess(x):\n",
    "    \"\"\"Hessian (2nd derivative): 2\"\"\"\n",
    "    return 2\n",
    "\n",
    "print(\"\\nLoss function: L(x) = (x - 3)² + 1\")\n",
    "print(\"Minimum at x = 3\")\n",
    "\n",
    "# Gradient Descent (1st order Taylor)\n",
    "print(\"\\n1. Gradient Descent (1st order approximation):\")\n",
    "print(\"-\"*60)\n",
    "\n",
    "x_gd = 0.0  # Starting point\n",
    "lr = 0.1    # Learning rate\n",
    "trajectory_gd = [x_gd]\n",
    "\n",
    "for i in range(10):\n",
    "    # 1st order update: x_new ≈ x_old - α·∇L\n",
    "    grad = loss_grad(x_gd)\n",
    "    x_gd = x_gd - lr * grad\n",
    "    trajectory_gd.append(x_gd)\n",
    "    \n",
    "    if i < 5 or i == 9:\n",
    "        print(f\"Step {i}: x = {x_gd:.4f}, L(x) = {loss(x_gd):.4f}\")\n",
    "\n",
    "# Newton's Method (2nd order Taylor)\n",
    "print(\"\\n2. Newton's Method (2nd order approximation):\")\n",
    "print(\"-\"*60)\n",
    "\n",
    "x_newton = 0.0  # Same starting point\n",
    "trajectory_newton = [x_newton]\n",
    "\n",
    "for i in range(5):\n",
    "    # 2nd order update: x_new = x_old - [∇L / ∇²L]\n",
    "    grad = loss_grad(x_newton)\n",
    "    hess = loss_hess(x_newton)\n",
    "    x_newton = x_newton - grad / hess\n",
    "    trajectory_newton.append(x_newton)\n",
    "    \n",
    "    print(f\"Step {i}: x = {x_newton:.4f}, L(x) = {loss(x_newton):.4f}\")\n",
    "\n",
    "print(\"\\n→ Gradient descent (1st order): slow, many steps\")\n",
    "print(\"→ Newton's method (2nd order): fast, fewer steps\")\n",
    "print(\"→ Trade-off: Newton requires Hessian (expensive!)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hessian-header",
   "metadata": {},
   "source": [
    "## 11. Hessian Matrix\n",
    "\n",
    "**Matrix of all second-order partial derivatives.**\n",
    "\n",
    "**For f: ℝⁿ → ℝ:**\n",
    "$$H_{ij} = \\frac{\\partial^2 f}{\\partial x_i \\partial x_j}$$\n",
    "\n",
    "**Properties:**\n",
    "- Symmetric matrix (Hᵀ = H)\n",
    "- Captures curvature of function\n",
    "- Eigenvalues determine local shape\n",
    "\n",
    "**ML Application:**\n",
    "- Newton's method\n",
    "- Identifying saddle points\n",
    "- Second-order optimization\n",
    "- Understanding loss landscape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hessian",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Hessian Matrix - Second Derivatives\n",
    "\n",
    "For f(x₁, x₂, ..., xₙ), Hessian H is:\n",
    "\n",
    "H = [∂²f/∂x₁²      ∂²f/∂x₁∂x₂  ...]\n",
    "    [∂²f/∂x₂∂x₁    ∂²f/∂x₂²    ...]\n",
    "    [...             ...        ...]\n",
    "\n",
    "Interpretation:\n",
    "- Diagonal: curvature along each axis\n",
    "- Off-diagonal: how axes interact\n",
    "- Eigenvalues: principal curvatures\n",
    "\n",
    "Positive definite H → local minimum\n",
    "Negative definite H → local maximum\n",
    "Mixed signs → saddle point\n",
    "\"\"\"\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"Hessian Matrix - Curvature Information\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Example: f(x, y) = x² + xy + y²\n",
    "print(\"\\nExample: f(x, y) = x² + xy + y²\")\n",
    "print(\"-\"*60)\n",
    "\n",
    "# Using SymPy for symbolic computation\n",
    "x, y = sp.symbols('x y')\n",
    "f_sym = x**2 + x*y + y**2\n",
    "\n",
    "# First derivatives (gradient)\n",
    "df_dx = sp.diff(f_sym, x)\n",
    "df_dy = sp.diff(f_sym, y)\n",
    "\n",
    "# Second derivatives (Hessian)\n",
    "d2f_dx2 = sp.diff(df_dx, x)\n",
    "d2f_dxdy = sp.diff(df_dx, y)\n",
    "d2f_dydx = sp.diff(df_dy, x)\n",
    "d2f_dy2 = sp.diff(df_dy, y)\n",
    "\n",
    "print(f\"Function: f(x, y) = {f_sym}\")\n",
    "print(f\"\\nGradient:\")\n",
    "print(f\"  ∂f/∂x = {df_dx}\")\n",
    "print(f\"  ∂f/∂y = {df_dy}\")\n",
    "\n",
    "print(f\"\\nHessian matrix:\")\n",
    "print(f\"  H = [∂²f/∂x²    ∂²f/∂x∂y]   [{d2f_dx2}  {d2f_dxdy}]\")\n",
    "print(f\"      [∂²f/∂y∂x   ∂²f/∂y²  ] = [{d2f_dydx}  {d2f_dy2}]\")\n",
    "\n",
    "# Numerical Hessian\n",
    "H = np.array([\n",
    "    [2, 1],\n",
    "    [1, 2]\n",
    "])\n",
    "\n",
    "print(f\"\\nSymmetric? H = Hᵀ? {np.allclose(H, H.T)} ✓\")\n",
    "\n",
    "# Eigenvalues determine shape\n",
    "eigenvalues, eigenvectors = np.linalg.eigh(H)\n",
    "\n",
    "print(f\"\\nEigenvalues: {eigenvalues}\")\n",
    "print(f\"All positive → positive definite → local MINIMUM ✓\")\n",
    "\n",
    "# Different examples\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Classifying Critical Points via Hessian\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "examples = [\n",
    "    (\"Minimum\", \"x² + y²\", np.array([[2, 0], [0, 2]])),\n",
    "    (\"Maximum\", \"-x² - y²\", np.array([[-2, 0], [0, -2]])),\n",
    "    (\"Saddle\", \"x² - y²\", np.array([[2, 0], [0, -2]])),\n",
    "]\n",
    "\n",
    "for name, func, hess in examples:\n",
    "    eigs = np.linalg.eigvalsh(hess)\n",
    "    \n",
    "    print(f\"\\n{name}: f(x, y) = {func}\")\n",
    "    print(f\"  Hessian = {hess.tolist()}\")\n",
    "    print(f\"  Eigenvalues: {eigs}\")\n",
    "    \n",
    "    if np.all(eigs > 0):\n",
    "        print(f\"  → All positive → LOCAL MINIMUM\")\n",
    "    elif np.all(eigs < 0):\n",
    "        print(f\"  → All negative → LOCAL MAXIMUM\")\n",
    "    else:\n",
    "        print(f\"  → Mixed signs → SADDLE POINT\")\n",
    "\n",
    "# ML Application\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ML Application: Hessian in Neural Networks\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\nWhy Hessian matters in deep learning:\")\n",
    "print(\"-\"*60)\n",
    "\n",
    "print(\"\\n1. Optimization Landscape:\")\n",
    "print(\"   - Positive definite → valley (good for optimization)\")\n",
    "print(\"   - Negative definite → peak (won't converge)\")\n",
    "print(\"   - Saddle points → common in high dimensions!\")\n",
    "\n",
    "print(\"\\n2. Second-Order Optimization:\")\n",
    "print(\"   - Newton's method: x_new = x_old - H⁻¹·∇f\")\n",
    "print(\"   - Uses curvature information\")\n",
    "print(\"   - Faster convergence than gradient descent\")\n",
    "print(\"   - But: computing H is O(n²) parameters!\")\n",
    "\n",
    "print(\"\\n3. Practical Approximations:\")\n",
    "print(\"   - Diagonal approximation (AdaGrad, RMSprop)\")\n",
    "print(\"   - Low-rank approximation (L-BFGS)\")\n",
    "print(\"   - Gauss-Newton, Levenberg-Marquardt\")\n",
    "\n",
    "# Demonstrate saddle point problem\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Demo: Saddle Point Challenge\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "def saddle_function(x, y):\n",
    "    \"\"\"Classic saddle: f(x,y) = x² - y²\"\"\"\n",
    "    return x**2 - y**2\n",
    "\n",
    "# At origin (0, 0):\n",
    "# - Gradient = [0, 0] (critical point)\n",
    "# - Hessian = [[2, 0], [0, -2]]\n",
    "# - Eigenvalues: [2, -2] → SADDLE\n",
    "\n",
    "print(\"\\nf(x, y) = x² - y²\")\n",
    "print(\"\\nAt origin (0, 0):\")\n",
    "print(\"  Gradient = [0, 0] ← looks like optimum!\")\n",
    "print(\"  But Hessian eigenvalues: [2, -2]\")\n",
    "print(\"  → Positive in x-direction (minimum)\")\n",
    "print(\"  → Negative in y-direction (maximum)\")\n",
    "print(\"  → SADDLE POINT (not a minimum!)\")\n",
    "\n",
    "print(\"\\n→ In deep learning: saddle points are COMMON\")\n",
    "print(\"→ High-dimensional spaces have many saddle points\")\n",
    "print(\"→ Gradient descent can escape them (unlike local minima)\")\n",
    "print(\"→ Hessian helps identify and understand them\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "linear-approx-header",
   "metadata": {},
   "source": [
    "## 12. Linear Approximation Using Gradient\n",
    "\n",
    "**First-order Taylor approximation:**\n",
    "\n",
    "$$f(x) \\approx f(x_0) + \\nabla f(x_0) \\cdot (x - x_0)$$\n",
    "\n",
    "**Interpretation:**\n",
    "- Tangent plane at x₀\n",
    "- Linear approximation valid LOCALLY\n",
    "- Basis for gradient descent\n",
    "\n",
    "**ML Application:**\n",
    "- Gradient descent step\n",
    "- Loss prediction after parameter update\n",
    "- Learning rate selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "linear-approx",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Linear Approximation - Gradient-Based Prediction\n",
    "\n",
    "Formula:\n",
    "f(x) ≈ f(x₀) + ∇f(x₀)·(x - x₀)\n",
    "\n",
    "This is:\n",
    "- 1st order Taylor expansion\n",
    "- Tangent plane approximation\n",
    "- Foundation of gradient descent\n",
    "\n",
    "Accuracy:\n",
    "- Good near x₀\n",
    "- Gets worse further away\n",
    "- Ignores curvature\n",
    "\"\"\"\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"Linear Approximation via Gradient\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Example function\n",
    "def f(x, y):\n",
    "    \"\"\"f(x, y) = x² + 2y² - xy\"\"\"\n",
    "    return x**2 + 2*y**2 - x*y\n",
    "\n",
    "def gradient_f(x, y):\n",
    "    \"\"\"∇f = [2x - y, 4y - x]\"\"\"\n",
    "    return np.array([2*x - y, 4*y - x])\n",
    "\n",
    "# Point of expansion\n",
    "x0 = np.array([1.0, 0.5])\n",
    "f0 = f(x0[0], x0[1])\n",
    "grad0 = gradient_f(x0[0], x0[1])\n",
    "\n",
    "print(f\"\\nFunction: f(x, y) = x² + 2y² - xy\")\n",
    "print(f\"\\nExpansion point: x₀ = {x0}\")\n",
    "print(f\"  f(x₀) = {f0:.4f}\")\n",
    "print(f\"  ∇f(x₀) = {grad0}\")\n",
    "\n",
    "# Linear approximation\n",
    "def linear_approx(x, y, x0, f0, grad0):\n",
    "    \"\"\"\n",
    "    Linear approximation:\n",
    "    f(x) ≈ f(x₀) + ∇f(x₀)·(x - x₀)\n",
    "    \"\"\"\n",
    "    x_vec = np.array([x, y])\n",
    "    dx = x_vec - x0\n",
    "    return f0 + np.dot(grad0, dx)\n",
    "\n",
    "# Test points at varying distances\n",
    "test_points = [\n",
    "    (1.0, 0.5),   # Same point\n",
    "    (1.1, 0.6),   # Close\n",
    "    (1.5, 1.0),   # Medium\n",
    "    (2.0, 1.5),   # Far\n",
    "]\n",
    "\n",
    "print(\"\\n\" + \"-\"*70)\n",
    "print(\"Approximation Quality vs Distance\")\n",
    "print(\"-\"*70)\n",
    "print(\"  Point         Distance   Exact      Approx     Error\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "for x, y in test_points:\n",
    "    exact = f(x, y)\n",
    "    approx = linear_approx(x, y, x0, f0, grad0)\n",
    "    distance = np.linalg.norm([x - x0[0], y - x0[1]])\n",
    "    error = abs(exact - approx)\n",
    "    \n",
    "    print(f\"({x:4.1f}, {y:4.1f})    {distance:8.3f}   {exact:8.3f}   {approx:8.3f}   {error:7.4f}\")\n",
    "\n",
    "print(\"\\n→ Closer to expansion point → better approximation\")\n",
    "print(\"→ Further away → linear model breaks down\")\n",
    "\n",
    "# Gradient Descent Application\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ML Application: Gradient Descent Step\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\nGradient descent update:\")\n",
    "print(\"  x_new = x_old - α·∇f(x_old)\")\n",
    "print(\"\\nThis comes from linear approximation!\")\n",
    "\n",
    "# Current parameters\n",
    "theta_old = np.array([2.0, 1.5])\n",
    "loss_old = f(theta_old[0], theta_old[1])\n",
    "grad = gradient_f(theta_old[0], theta_old[1])\n",
    "\n",
    "print(f\"\\nCurrent: θ = {theta_old}, Loss = {loss_old:.4f}\")\n",
    "print(f\"Gradient: ∇Loss = {grad}\")\n",
    "\n",
    "# Try different learning rates\n",
    "learning_rates = [0.01, 0.05, 0.1, 0.2, 0.5]\n",
    "\n",
    "print(\"\\n\" + \"-\"*70)\n",
    "print(\"Testing different learning rates:\")\n",
    "print(\"-\"*70)\n",
    "print(\"  α      θ_new         Loss(actual)  Loss(predicted)  Error\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "for alpha in learning_rates:\n",
    "    # Gradient descent step\n",
    "    theta_new = theta_old - alpha * grad\n",
    "    \n",
    "    # Actual loss\n",
    "    loss_actual = f(theta_new[0], theta_new[1])\n",
    "    \n",
    "    # Predicted loss (linear approximation)\n",
    "    # L(θ_new) ≈ L(θ_old) + ∇L·(θ_new - θ_old)\n",
    "    #          = L(θ_old) + ∇L·(-α∇L)\n",
    "    #          = L(θ_old) - α||∇L||²\n",
    "    loss_predicted = loss_old - alpha * np.dot(grad, grad)\n",
    "    \n",
    "    error = abs(loss_actual - loss_predicted)\n",
    "    \n",
    "    print(f\"{alpha:5.2f}  [{theta_new[0]:5.2f}, {theta_new[1]:5.2f}]  \"\n",
    "          f\"{loss_actual:11.4f}  {loss_predicted:14.4f}  {error:8.4f}\")\n",
    "\n",
    "print(\"\\n→ Small α: prediction accurate (staying in linear region)\")\n",
    "print(\"→ Large α: prediction breaks down (leaving linear region)\")\n",
    "print(\"→ This is why learning rate tuning matters!\")\n",
    "\n",
    "# Summary\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Key Insights\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\n1. Linear approximation is LOCAL\")\n",
    "print(\"   - Valid only near expansion point\")\n",
    "print(\"   - Breaks down further away\")\n",
    "\n",
    "print(\"\\n2. Gradient descent assumes linearity\")\n",
    "print(\"   - Each step uses linear approximation\")\n",
    "print(\"   - Learning rate controls step size\")\n",
    "print(\"   - Too large → leave linear region → unstable\")\n",
    "\n",
    "print(\"\\n3. Curvature matters\")\n",
    "print(\"   - Linear approximation ignores curvature\")\n",
    "print(\"   - Second-order methods (Newton) use Hessian\")\n",
    "print(\"   - Better approximation but more expensive\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary-header",
   "metadata": {},
   "source": [
    "## 13. Complete Summary & Common Mistakes\n",
    "\n",
    "A comprehensive review of all vector calculus concepts for ML."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "summary",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "VECTOR CALCULUS FOR ML - COMPLETE SUMMARY\n",
      "======================================================================\n",
      "\n",
      "Key Concepts:\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "1. Difference Quotient\n",
      "   Formula: [f(x+h) - f(x)] / h\n",
      "   ML Use:  Numerical gradients\n",
      "\n",
      "2. Derivative\n",
      "   Formula: lim(h→0) [f(x+h)-f(x)]/h\n",
      "   ML Use:  Parameter sensitivity\n",
      "\n",
      "3. Partial Derivative\n",
      "   Formula: ∂f/∂xᵢ (others constant)\n",
      "   ML Use:  Per-parameter gradients\n",
      "\n",
      "4. Gradient\n",
      "   Formula: ∇f = [∂f/∂x₁, ..., ∂f/∂xₙ]ᵀ\n",
      "   ML Use:  Gradient descent direction\n",
      "\n",
      "5. Jacobian\n",
      "   Formula: Jᵢⱼ = ∂fᵢ/∂xⱼ\n",
      "   ML Use:  Transformations\n",
      "\n",
      "6. Hessian\n",
      "   Formula: Hᵢⱼ = ∂²f/∂xᵢ∂xⱼ\n",
      "   ML Use:  Curvature, Newton's method\n",
      "\n",
      "7. Sum Rule\n",
      "   Formula: (f+g)' = f' + g'\n",
      "   ML Use:  Multi-term losses\n",
      "\n",
      "8. Product Rule\n",
      "   Formula: (fg)' = f'g + fg'\n",
      "   ML Use:  Weighted combinations\n",
      "\n",
      "9. Chain Rule\n",
      "   Formula: (f∘g)' = f'(g)·g'\n",
      "   ML Use:  BACKPROPAGATION\n",
      "\n",
      "10. Taylor Series\n",
      "   Formula: f(x) ≈ Σ f⁽ⁿ⁾(a)(x-a)ⁿ/n!\n",
      "   ML Use:  Local approximations\n",
      "\n",
      "11. Linear Approximation\n",
      "   Formula: f(x) ≈ f(x₀) + ∇f(x₀)·(x-x₀)\n",
      "   ML Use:  GD step prediction\n",
      "\n",
      "======================================================================\n",
      "⚠️  COMMON MISTAKES TO AVOID\n",
      "======================================================================\n",
      "\n",
      "1. Confusing gradient and Jacobian\n",
      "   ✗ Using gradient for vector outputs\n",
      "   ✓ Gradient: scalar output | Jacobian: vector output\n",
      "\n",
      "2. Forgetting transpose in gradients\n",
      "   ✗ Treating gradient as row vector\n",
      "   ✓ Gradient is COLUMN vector: ∇f = [∂f/∂x₁, ...]ᵀ\n",
      "\n",
      "3. Ignoring higher-order terms\n",
      "   ✗ Assuming linear approximation is always accurate\n",
      "   ✓ Linear approx only valid LOCALLY (small steps)\n",
      "\n",
      "4. Not checking convexity\n",
      "   ✗ Assuming gradient descent finds global minimum\n",
      "   ✓ Non-convex → local minima, saddle points possible\n",
      "\n",
      "5. Wrong chain rule application\n",
      "   ✗ Forgetting intermediate derivatives\n",
      "   ✓ Chain ALL derivatives: df/dx = df/du · du/dx\n",
      "\n",
      "6. Numerical instability\n",
      "   ✗ h too small in difference quotient → precision errors\n",
      "   ✓ Use h ≈ 1e-8, or automatic differentiation\n",
      "\n",
      "======================================================================\n",
      "🎉 CONGRATULATIONS!\n",
      "======================================================================\n",
      "\n",
      "You now understand:\n",
      "  ✓ How derivatives enable learning\n",
      "  ✓ Why gradients point uphill (and -∇f downhill)\n",
      "  ✓ How backpropagation works (chain rule!)\n",
      "  ✓ The math behind gradient descent\n",
      "  ✓ Why curvature matters (Hessian)\n",
      "  ✓ How to approximate functions (Taylor series)\n",
      "\n",
      "You're ready to:\n",
      "  → Implement gradient descent from scratch\n",
      "  → Understand neural network training\n",
      "  → Debug optimization issues\n",
      "  → Read ML research papers\n",
      "\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Complete Vector Calculus Summary for ML\n",
    "\"\"\"\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"VECTOR CALCULUS FOR ML - COMPLETE SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "summary_table = {\n",
    "    \"Concept\": [\n",
    "        \"Difference Quotient\",\n",
    "        \"Derivative\",\n",
    "        \"Partial Derivative\",\n",
    "        \"Gradient\",\n",
    "        \"Jacobian\",\n",
    "        \"Hessian\",\n",
    "        \"Sum Rule\",\n",
    "        \"Product Rule\",\n",
    "        \"Chain Rule\",\n",
    "        \"Taylor Series\",\n",
    "        \"Linear Approximation\"\n",
    "    ],\n",
    "    \"Formula\": [\n",
    "        \"[f(x+h) - f(x)] / h\",\n",
    "        \"lim(h→0) [f(x+h)-f(x)]/h\",\n",
    "        \"∂f/∂xᵢ (others constant)\",\n",
    "        \"∇f = [∂f/∂x₁, ..., ∂f/∂xₙ]ᵀ\",\n",
    "        \"Jᵢⱼ = ∂fᵢ/∂xⱼ\",\n",
    "        \"Hᵢⱼ = ∂²f/∂xᵢ∂xⱼ\",\n",
    "        \"(f+g)' = f' + g'\",\n",
    "        \"(fg)' = f'g + fg'\",\n",
    "        \"(f∘g)' = f'(g)·g'\",\n",
    "        \"f(x) ≈ Σ f⁽ⁿ⁾(a)(x-a)ⁿ/n!\",\n",
    "        \"f(x) ≈ f(x₀) + ∇f(x₀)·(x-x₀)\"\n",
    "    ],\n",
    "    \"ML Use\": [\n",
    "        \"Numerical gradients\",\n",
    "        \"Parameter sensitivity\",\n",
    "        \"Per-parameter gradients\",\n",
    "        \"Gradient descent direction\",\n",
    "        \"Transformations\",\n",
    "        \"Curvature, Newton's method\",\n",
    "        \"Multi-term losses\",\n",
    "        \"Weighted combinations\",\n",
    "        \"BACKPROPAGATION\",\n",
    "        \"Local approximations\",\n",
    "        \"GD step prediction\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "print(\"\\nKey Concepts:\")\n",
    "print(\"-\"*70)\n",
    "for i in range(len(summary_table[\"Concept\"])):\n",
    "    print(f\"\\n{i+1}. {summary_table['Concept'][i]}\")\n",
    "    print(f\"   Formula: {summary_table['Formula'][i]}\")\n",
    "    print(f\"   ML Use:  {summary_table['ML Use'][i]}\")\n",
    "\n",
    "# Common Mistakes\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"⚠️  COMMON MISTAKES TO AVOID\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "mistakes = [\n",
    "    (\n",
    "        \"1. Confusing gradient and Jacobian\",\n",
    "        \"   ✗ Using gradient for vector outputs\",\n",
    "        \"   ✓ Gradient: scalar output | Jacobian: vector output\"\n",
    "    ),\n",
    "    (\n",
    "        \"2. Forgetting transpose in gradients\",\n",
    "        \"   ✗ Treating gradient as row vector\",\n",
    "        \"   ✓ Gradient is COLUMN vector: ∇f = [∂f/∂x₁, ...]ᵀ\"\n",
    "    ),\n",
    "    (\n",
    "        \"3. Ignoring higher-order terms\",\n",
    "        \"   ✗ Assuming linear approximation is always accurate\",\n",
    "        \"   ✓ Linear approx only valid LOCALLY (small steps)\"\n",
    "    ),\n",
    "    (\n",
    "        \"4. Not checking convexity\",\n",
    "        \"   ✗ Assuming gradient descent finds global minimum\",\n",
    "        \"   ✓ Non-convex → local minima, saddle points possible\"\n",
    "    ),\n",
    "    (\n",
    "        \"5. Wrong chain rule application\",\n",
    "        \"   ✗ Forgetting intermediate derivatives\",\n",
    "        \"   ✓ Chain ALL derivatives: df/dx = df/du · du/dx\"\n",
    "    ),\n",
    "    (\n",
    "        \"6. Numerical instability\",\n",
    "        \"   ✗ h too small in difference quotient → precision errors\",\n",
    "        \"   ✓ Use h ≈ 1e-8, or automatic differentiation\"\n",
    "    )\n",
    "]\n",
    "\n",
    "for wrong, bad, good in mistakes:\n",
    "    print(f\"\\n{wrong}\")\n",
    "    print(bad)\n",
    "    print(good)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"🎉 CONGRATULATIONS!\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nYou now understand:\")\n",
    "print(\"  ✓ How derivatives enable learning\")\n",
    "print(\"  ✓ Why gradients point uphill (and -∇f downhill)\")\n",
    "print(\"  ✓ How backpropagation works (chain rule!)\")\n",
    "print(\"  ✓ The math behind gradient descent\")\n",
    "print(\"  ✓ Why curvature matters (Hessian)\")\n",
    "print(\"  ✓ How to approximate functions (Taylor series)\")\n",
    "print(\"\\nYou're ready to:\")\n",
    "print(\"  → Implement gradient descent from scratch\")\n",
    "print(\"  → Understand neural network training\")\n",
    "print(\"  → Debug optimization issues\")\n",
    "print(\"  → Read ML research papers\")\n",
    "print(\"\\n\" + \"=\"*70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
